{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlike-portal",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "assisted-protocol",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "classical_models.ipynb\n",
    "\n",
    "by: Archie Gertsman (arkadiy2@illinois.edu)\n",
    "Lloyd Fernandes (lloydf2@illinois.edu)\n",
    "\n",
    "Project director: Richard Sowers\n",
    "\n",
    "r-sowers@illinois.eduhttps://publish.illinois.edu/r-sowers/\n",
    "\n",
    "Copyright 2019 University of Illinois Board of Trustees. All Rights Reserved. Licensed under the MIT license\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "introductory-panic",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings; warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "alert-resort",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../Lib/')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from feature_eng import split_trajectories\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from time import time\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from IPython.display import display\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import RFE\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "functioning-cyprus",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>speed</th>\n",
       "      <th>lon_acc</th>\n",
       "      <th>lat_acc</th>\n",
       "      <th>type</th>\n",
       "      <th>traveled_d</th>\n",
       "      <th>avg_speed</th>\n",
       "      <th>bearing</th>\n",
       "      <th>nearest_edge_start_node</th>\n",
       "      <th>...</th>\n",
       "      <th>edge_progress_intervals</th>\n",
       "      <th>len</th>\n",
       "      <th>lanes</th>\n",
       "      <th>node_veh_dist</th>\n",
       "      <th>edge_seg</th>\n",
       "      <th>vehicle_density</th>\n",
       "      <th>avg_surr_speed</th>\n",
       "      <th>edge_bearing</th>\n",
       "      <th>acc_edge</th>\n",
       "      <th>acc_per_edge</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>file_name</th>\n",
       "      <th>id</th>\n",
       "      <th>edge_id</th>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">4_1</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">1</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">250699362_250699984</th>\n",
       "      <th>42.00</th>\n",
       "      <td>37.982746</td>\n",
       "      <td>23.732961</td>\n",
       "      <td>11.9046</td>\n",
       "      <td>-0.1145</td>\n",
       "      <td>0.0138</td>\n",
       "      <td>Taxi</td>\n",
       "      <td>182.37</td>\n",
       "      <td>9.740748</td>\n",
       "      <td>1.570795</td>\n",
       "      <td>250699362</td>\n",
       "      <td>...</td>\n",
       "      <td>0.3</td>\n",
       "      <td>97.581</td>\n",
       "      <td>5.4</td>\n",
       "      <td>29.814330</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>10.464171</td>\n",
       "      <td>-2.83013</td>\n",
       "      <td>0.113220</td>\n",
       "      <td>0.021953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42.04</th>\n",
       "      <td>37.982746</td>\n",
       "      <td>23.732963</td>\n",
       "      <td>11.8975</td>\n",
       "      <td>-0.1007</td>\n",
       "      <td>0.0147</td>\n",
       "      <td>Taxi</td>\n",
       "      <td>182.37</td>\n",
       "      <td>9.740748</td>\n",
       "      <td>0.168572</td>\n",
       "      <td>250699362</td>\n",
       "      <td>...</td>\n",
       "      <td>0.3</td>\n",
       "      <td>97.581</td>\n",
       "      <td>5.4</td>\n",
       "      <td>29.674830</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>10.457843</td>\n",
       "      <td>-2.83013</td>\n",
       "      <td>0.100360</td>\n",
       "      <td>0.016867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42.08</th>\n",
       "      <td>37.982747</td>\n",
       "      <td>23.732964</td>\n",
       "      <td>11.8919</td>\n",
       "      <td>-0.0918</td>\n",
       "      <td>0.0157</td>\n",
       "      <td>Taxi</td>\n",
       "      <td>182.37</td>\n",
       "      <td>9.740748</td>\n",
       "      <td>0.168573</td>\n",
       "      <td>250699362</td>\n",
       "      <td>...</td>\n",
       "      <td>0.3</td>\n",
       "      <td>97.581</td>\n",
       "      <td>5.4</td>\n",
       "      <td>29.537753</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>10.452857</td>\n",
       "      <td>-2.83013</td>\n",
       "      <td>0.092194</td>\n",
       "      <td>0.013188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42.12</th>\n",
       "      <td>37.982748</td>\n",
       "      <td>23.732965</td>\n",
       "      <td>11.8871</td>\n",
       "      <td>-0.0869</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>Taxi</td>\n",
       "      <td>182.37</td>\n",
       "      <td>9.740748</td>\n",
       "      <td>1.570796</td>\n",
       "      <td>250699362</td>\n",
       "      <td>...</td>\n",
       "      <td>0.3</td>\n",
       "      <td>97.581</td>\n",
       "      <td>5.4</td>\n",
       "      <td>29.400718</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>10.448586</td>\n",
       "      <td>-2.83013</td>\n",
       "      <td>0.087837</td>\n",
       "      <td>0.010734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42.16</th>\n",
       "      <td>37.982748</td>\n",
       "      <td>23.732966</td>\n",
       "      <td>11.8831</td>\n",
       "      <td>-0.0784</td>\n",
       "      <td>0.0176</td>\n",
       "      <td>Taxi</td>\n",
       "      <td>182.37</td>\n",
       "      <td>9.740748</td>\n",
       "      <td>0.328080</td>\n",
       "      <td>250699362</td>\n",
       "      <td>...</td>\n",
       "      <td>0.3</td>\n",
       "      <td>97.581</td>\n",
       "      <td>5.4</td>\n",
       "      <td>29.330986</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>10.444986</td>\n",
       "      <td>-2.83013</td>\n",
       "      <td>0.080021</td>\n",
       "      <td>0.007273</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              lat        lon    speed  \\\n",
       "file_name id edge_id             time                                   \n",
       "4_1       1  250699362_250699984 42.00  37.982746  23.732961  11.9046   \n",
       "                                 42.04  37.982746  23.732963  11.8975   \n",
       "                                 42.08  37.982747  23.732964  11.8919   \n",
       "                                 42.12  37.982748  23.732965  11.8871   \n",
       "                                 42.16  37.982748  23.732966  11.8831   \n",
       "\n",
       "                                        lon_acc  lat_acc  type  traveled_d  \\\n",
       "file_name id edge_id             time                                        \n",
       "4_1       1  250699362_250699984 42.00  -0.1145   0.0138  Taxi      182.37   \n",
       "                                 42.04  -0.1007   0.0147  Taxi      182.37   \n",
       "                                 42.08  -0.0918   0.0157  Taxi      182.37   \n",
       "                                 42.12  -0.0869   0.0167  Taxi      182.37   \n",
       "                                 42.16  -0.0784   0.0176  Taxi      182.37   \n",
       "\n",
       "                                        avg_speed   bearing  \\\n",
       "file_name id edge_id             time                         \n",
       "4_1       1  250699362_250699984 42.00   9.740748  1.570795   \n",
       "                                 42.04   9.740748  0.168572   \n",
       "                                 42.08   9.740748  0.168573   \n",
       "                                 42.12   9.740748  1.570796   \n",
       "                                 42.16   9.740748  0.328080   \n",
       "\n",
       "                                        nearest_edge_start_node  ...  \\\n",
       "file_name id edge_id             time                            ...   \n",
       "4_1       1  250699362_250699984 42.00                250699362  ...   \n",
       "                                 42.04                250699362  ...   \n",
       "                                 42.08                250699362  ...   \n",
       "                                 42.12                250699362  ...   \n",
       "                                 42.16                250699362  ...   \n",
       "\n",
       "                                        edge_progress_intervals     len  \\\n",
       "file_name id edge_id             time                                     \n",
       "4_1       1  250699362_250699984 42.00                      0.3  97.581   \n",
       "                                 42.04                      0.3  97.581   \n",
       "                                 42.08                      0.3  97.581   \n",
       "                                 42.12                      0.3  97.581   \n",
       "                                 42.16                      0.3  97.581   \n",
       "\n",
       "                                        lanes  node_veh_dist  edge_seg  \\\n",
       "file_name id edge_id             time                                    \n",
       "4_1       1  250699362_250699984 42.00    5.4      29.814330       1.0   \n",
       "                                 42.04    5.4      29.674830       1.0   \n",
       "                                 42.08    5.4      29.537753       1.0   \n",
       "                                 42.12    5.4      29.400718       1.0   \n",
       "                                 42.16    5.4      29.330986       1.0   \n",
       "\n",
       "                                        vehicle_density  avg_surr_speed  \\\n",
       "file_name id edge_id             time                                     \n",
       "4_1       1  250699362_250699984 42.00                7       10.464171   \n",
       "                                 42.04                7       10.457843   \n",
       "                                 42.08                7       10.452857   \n",
       "                                 42.12                7       10.448586   \n",
       "                                 42.16                7       10.444986   \n",
       "\n",
       "                                        edge_bearing  acc_edge  acc_per_edge  \n",
       "file_name id edge_id             time                                         \n",
       "4_1       1  250699362_250699984 42.00      -2.83013  0.113220      0.021953  \n",
       "                                 42.04      -2.83013  0.100360      0.016867  \n",
       "                                 42.08      -2.83013  0.092194      0.013188  \n",
       "                                 42.12      -2.83013  0.087837      0.010734  \n",
       "                                 42.16      -2.83013  0.080021      0.007273  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle('block4_concat_lane.pkl')  \\\n",
    "    .set_index('edge_id', append=True) \\\n",
    "    .reorder_levels((0,1,3,2))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "angry-shame",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_agg(df, agg_dict, window_size=100, step=25):\n",
    "    # rolling agg with step size = 1\n",
    "    df_agg = df.groupby(df.index.names[:-1]) \\\n",
    "                .rolling(window_size) \\\n",
    "                .agg(agg_dict) \\\n",
    "                .dropna()\n",
    "    #print(df_agg)\n",
    "    # select a subset of above computations to achieve custom step size\n",
    "    df_agg = df_agg.groupby(df_agg.index.names, \n",
    "                            as_index=False, \n",
    "                            group_keys=False) \\\n",
    "                .apply(lambda x: x[::step])\n",
    "    #print(df_agg)\n",
    "    df_agg.columns = ['_'.join(col) for col in df_agg.columns]\n",
    "    \n",
    "    \n",
    "    # add 'type' column\n",
    "    vehicle_types = df.type.groupby(df.index.names[:-1]).first()\n",
    "    #print(vehicle_types)\n",
    "    return df_agg.join(vehicle_types)\n",
    "  \n",
    "def speed_ratio(grp, min_speed=0):\n",
    "   \n",
    "    return len(grp[grp.speed > min_speed]) / len(grp)\n",
    "\n",
    "def validation_set(df,test_size):\n",
    "    \"\"\"dataframe is split based on their vehicle id's\"\"\"\n",
    "    df_val = df.reset_index()[[\"file_name\",'id','type']].drop_duplicates()\n",
    "    X,y = df_val[[\"file_name\",\"id\"]],df_val['type']\n",
    "    X_train,X_test,_,y_test = train_test_split(X, y, test_size=test_size, random_state=4, stratify=y) \n",
    "    df_train = df[df.index.droplevel(['time','edge_id']).isin(X_train.set_index(['file_name','id']).index)]\n",
    "    X_test['type'] = y_test\n",
    "    g = X_test.groupby('type')\n",
    "    X_test = g.apply(lambda group: group.sample(g.size().min())).reset_index(drop = True)\n",
    "    df_test = df[df.index.droplevel(['time','edge_id']).isin(X_test.set_index(['file_name','id']).index)]\n",
    "    return df_train,df_test\n",
    "\n",
    "def train_and_accuracy(X_test,y_test, model):\n",
    "\n",
    "    #model.fit(X_train, y_train)\n",
    "    y_hat = model.predict(X_test)\n",
    "    a = y_hat==y_test\n",
    "    \n",
    "    f = f1_score((y_test == 'Car').astype(int),(y_hat == 'Car').astype(int))\n",
    "    return len(a[a==True]) / len(y_test),f\n",
    "\n",
    "def val_voting_accuracy(X_val,y_val, model,by_edge = False,predict_proba = False,display = False):\n",
    "\n",
    "    if predict_proba == True:\n",
    "        \n",
    "        y_hat = pd.DataFrame(index = y_val.index,data = model.predict_proba(X_val),columns = model.classes_)\n",
    "        y_hat_orig = y_hat.copy()\n",
    "        #print(y_hat_orig)\n",
    "        if by_edge == False:\n",
    "            \n",
    "            #predicted value for the entire trajectory would be the mode of the predicted labels\n",
    "            y_hat = y_hat.groupby(['file_name','id']).mean()\n",
    "            y_hat = y_hat.idxmax(axis=1)#.to_numpy()\n",
    "            y_test = y_val.groupby(['file_name','id']).first(['type'])\n",
    "             \n",
    "        else:\n",
    "\n",
    "            #predicted value for the entire trajectory would be the mode of the predicted labels\n",
    "            y_hat = y_hat.groupby(['file_name','id','edge_id']).mean()\n",
    "            y_hat = y_hat.idxmax(axis=1)#.to_numpy()\n",
    "            y_test = y_val.groupby(['file_name','id','edge_id']).first(['type'])\n",
    "\n",
    "    else:\n",
    "        y_hat = model.predict(X_val)\n",
    "        y_hat = pd.DataFrame(index = y_val.index,data = y_hat,columns = ['type'])\n",
    "        y_hat_orig = y_hat.copy()\n",
    "        if by_edge == False:\n",
    "            \n",
    "            #predicted value for the entire trajectory would be the mode of the predicted labels\n",
    "            y_hat = y_hat.groupby(['file_name','id']).apply(lambda group: pd.Series.mode(group['type'])[0])\n",
    "            y_test = y_val.groupby(['file_name','id']).first(['type'])\n",
    "        else:\n",
    "            \n",
    "            #predicted value for the entire trajectory would be the mode of the predicted labels\n",
    "            y_hat = y_hat.groupby(['file_name','id','edge_id']).apply(lambda group: pd.Series.mode(group['type'])[0])\n",
    "            y_test = y_val.groupby(['file_name','id','edge_id']).first(['type'])\n",
    "\n",
    "    if display:\n",
    "        y_hat_orig['id_traj'] = list(range(len(y_hat_orig)))\n",
    "        #y_hat_orig.set_index(['id_traj'], inplace = True,append = True)\n",
    "        \n",
    "        x_plot_num = 5\n",
    "        y_plot_num = int(sum(y_hat!=y_test)/x_plot_num) +1\n",
    "        fig, axes = plt.subplots(y_plot_num,x_plot_num, sharey = True, figsize=(5*x_plot_num,5*(y_plot_num)))\n",
    "        axes = axes.ravel()\n",
    "        i = 0\n",
    "        \n",
    "        for file_name,idx in X_val.index.droplevel((2)).unique():\n",
    "            if str(y_hat.loc[(file_name,idx)]) == str(y_test.loc[(file_name,idx)]):\n",
    "                continue\n",
    "            \n",
    "            axes[i].set(ylim=(0,1))\n",
    "            type_str = \"predicted: \"+str(y_hat.loc[(file_name,idx)]) + \", actual: \"+str(y_test.loc[(file_name,idx)])\n",
    "            sns.barplot(y = 'Car',x = 'id_traj',data = y_hat_orig.loc[(file_name,idx)],ax = axes[i]).set_title(\"file_name: \"+str(file_name)+\", id \"+str(idx)+\" \\n \"+type_str)\n",
    "            i+=1\n",
    "            \n",
    "        fig.tight_layout(h_pad=2)\n",
    "        #plt.show()\n",
    "        \n",
    "        \n",
    "    a = y_hat==y_test\n",
    "   \n",
    "    f = f1_score((y_test == 'Car').astype(int),(y_hat == 'Car').astype(int))\n",
    "   \n",
    "    return len(a[a==True]) / len(y_test),f\n",
    "            \n",
    "#val_voting_accuracy(X_val,y_val, model,predict_proba = True, display = True)       \n",
    "#plt.savefig('fig.png',dpi = 100)       \n",
    "\n",
    "def get_xy(df,overlap = None,traj_len = None,agg_dict = None,min_movement_limit = 0.75,outlier_limit=None,balance = None,downsample_feature_list = None,col_factor = None,window = None):\n",
    "    \n",
    "    if agg_dict is not None:\n",
    "        df_agg =rolling_agg(df, window_size=traj_len, step=int((1 - overlap)*traj_len),agg_dict = agg_dict)\n",
    "        df_agg = df_agg[df_agg.speed_bool_count*min_movement_limit <= df_agg.speed_bool_sum]\n",
    "        df_agg.drop(['speed_bool_count','speed_bool_sum'],inplace= True,axis = 1)\n",
    "        if outlier_limit is not None:\n",
    "            df_agg = filter_by_percentile(df_agg,outlier_limit)\n",
    "        if balance == 'by_edge':\n",
    "\n",
    "            df_agg['type_count'] = df_agg['type']\n",
    "            g_count = df_agg.groupby(['edge_id','type'], group_keys=False).count()['type_count']\n",
    "            g = df_agg.groupby(['type','edge_id'], group_keys=False)\n",
    "            df_agg = g.apply(lambda grp: grp.sample(min(g_count.loc[(grp.index.get_level_values(2)[0],slice(None))])))\n",
    "            df_agg.drop('type_count',inplace = True,axis = 1)\n",
    "\n",
    "        if balance == 'by_type':\n",
    "            g = df_agg.groupby('type', group_keys=False)\n",
    "            df_agg = g.apply(lambda grp: grp.sample(g.size().min()))\n",
    "            \n",
    "    elif downsample_feature_list is not None:\n",
    "        \n",
    "        df_agg = downsample(df,downsample_feature_list,col_factor,window)\n",
    "        \n",
    "        if outlier_limit is not None:\n",
    "            df_agg = filter_by_percentile(df_agg,outlier_limit)\n",
    "        if balance == 'by_edge':\n",
    "\n",
    "            df_agg['type_count'] = df_agg['type']\n",
    "            g_count = df_agg.groupby(['edge_id','type'], group_keys=False).count()['type_count']\n",
    "            g = df_agg.groupby(['type','edge_id'], group_keys=False)\n",
    "            df_agg = g.apply(lambda grp: grp.sample(min(g_count.loc[(grp.index.get_level_values(2)[0],slice(None))])))\n",
    "            df_agg.drop('type_count',inplace = True,axis = 1)\n",
    "\n",
    "        if balance == 'by_type':\n",
    "            g = df_agg.groupby('type', group_keys=False)\n",
    "            df_agg = g.apply(lambda grp: grp.sample(g.size().min()))\n",
    "        \n",
    "    X,y = df_agg.drop('type', axis=1), df_agg.type\n",
    "    return X,y\n",
    "  \n",
    "def filter_by_percentile(df,percentile):\n",
    "    \n",
    "    top_le = 1-(percentile/100)\n",
    "    bottom_le = percentile/100\n",
    "    df_top = df.quantile(top_le).reset_index()\n",
    "    df_top['cond'] ='('+df_top['index']+\" <= \"+df_top[top_le].astype(str)+')'\n",
    "    df_bottom = df.quantile(bottom_le).reset_index()\n",
    "    df_bottom['cond'] ='('+df_bottom['index']+\" >= \"+df_bottom[bottom_le].astype(str)+')'\n",
    "    df = df.query(df_top.cond.str.cat(sep=' & '))\n",
    "    df = df.query(df_bottom.cond.str.cat(sep=' & '))\n",
    "    \n",
    "    return df  \n",
    "\n",
    "def __xtrack_dist_diff(df):\n",
    "    \"\"\"splits a vehicle trajectory into smaller trajectories of fixed size and removes\n",
    "    the last (len(df) mod size) riws\n",
    "    \"\"\"\n",
    "    \n",
    "    #df[\"xtrack_diff\"] = df.loc[:,['xtrack_dist']]- df.loc[:,['xtrack_dist']].shift(-1)\n",
    "    #df[\"xtrack_diff\"]=df['xtrack_diff'].fillna(0)\n",
    "    df['xtrack_diff'] = df.xtrack_dist \\\n",
    "    .groupby(df.index.names[-1]) \\\n",
    "    .apply(lambda x: (x - x.shift(-1)).fillna(0))\n",
    "    \n",
    "    return df\n",
    "\n",
    "def pivot(A, col_factor):\n",
    "    c = A.shape[1]\n",
    "    if A.size < col_factor*c:\n",
    "        return None\n",
    "    r_new = A.size // (col_factor*c)\n",
    "    A = A[:col_factor*r_new]\n",
    "    return A.to_numpy().reshape(r_new, col_factor*c)\n",
    "\n",
    "def f(grp,window,col_factor):\n",
    "    grp = grp.reset_index(level=(0,1,2), drop=True)\n",
    "    grp.index = pd.TimedeltaIndex(grp.index,unit='s')\n",
    "    grp = grp.resample(window).mean().reset_index(drop=True)\n",
    "    return pd.DataFrame(pivot(grp,col_factor))\n",
    "\n",
    "\n",
    "def downsample(df,feature_list,col_factor,window):\n",
    "    df_lane_len = df[['len','lanes','type']].droplevel(3).reset_index().drop_duplicates().set_index(df.index.names[:-1])\n",
    "    df = df[feature_list] \\\n",
    "        .groupby(df[feature_list].index.names[:-1]) \\\n",
    "        .apply(lambda grp: f(grp,window,col_factor)) \\\n",
    "        .dropna() \\\n",
    "        .reset_index(level=-1, drop=True)\n",
    "   # print(df)\n",
    "    df.columns = [feature+'_'+str(i) for i in range(col_factor) for feature in feature_list]\n",
    "   \n",
    "    df = df.join(df_lane_len)\n",
    "    \n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "moving-meeting",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = __xtrack_dist_diff(df)\n",
    "#df['xtrack_diff_sq'] = df['xtrack_diff']**2\n",
    "#df['acc_edge_sq'] = df['acc_edge']**2\n",
    "#df['acc_per_edge_sq'] = df['acc_per_edge']**2\n",
    "#df['vehicle_density_by_lane'] = df['vehicle_density']/df['lanes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "remarkable-malta",
   "metadata": {},
   "outputs": [],
   "source": [
    "class voting_model():\n",
    "    def __init__(self,model,X,y):\n",
    "        self.model = model\n",
    "        self.voting_model = self.fit(X,y)\n",
    "        \n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        \"\"\"fit quadratic weighted function on model output using X,y\"\"\"\n",
    "        \n",
    "        model_output = self.generate_op_df(X)\n",
    "        X_log = model_output.groupby(['file_name','id']).mean()\n",
    "        \n",
    "        Y_log = y.groupby(['file_name','id']).first(['type']).apply(lambda x: 1 if (self.model.classes_[0] == x) else -1 )\n",
    "        \n",
    "        model = LogisticRegression(penalty = 'none')\n",
    "        return model.fit(X_log,Y_log)\n",
    "    \n",
    "    \n",
    "    def generate_op_df(self,X):\n",
    "        model_output = pd.DataFrame(data = self.model.predict_proba(X)[:,0],index = X.index,columns = ['x_1'])\n",
    "        \n",
    "        model_output['x_2'] = model_output['x_1']**2\n",
    "        model_output['x_3'] = model_output['x_1']**3\n",
    "        model_output['const'] = 1\n",
    "        model_output['x_4'] = model_output['x_1']**4\n",
    "        \n",
    "        return model_output\n",
    "    \n",
    "    def predict(self,X):\n",
    "        model_output = self.generate_op_df(X)\n",
    "        X_test = model_output.groupby(['file_name','id']).mean()\n",
    "        model_output = self.voting_model.predict(X_test)\n",
    "        model_output = np.vectorize(lambda x: self.model.classes_[0] if (x>=0) else self.model.classes_[1])(model_output)\n",
    "        \n",
    "        return model_output\n",
    "    \n",
    "    def accuracy(self,X,y):\n",
    "        y_test = y.groupby(['file_name','id']).first(['type'])\n",
    "        y_hat = self.predict(X)\n",
    "        y_hat = pd.DataFrame(index = y_test.index,data = y_hat,columns = ['type'])\n",
    "        \n",
    "        #predicted value for the entire trajectory would be the mode of the predicted labels\n",
    "        #y_hat = y_hat.groupby(['file_name','id']).apply(lambda group: pd.Series.mode(group['type'])[0])\n",
    "        #y_test = y_val.groupby(['file_name','id']).first(['type'])\n",
    "        \n",
    "        a = y_hat['type']==y_test\n",
    "   \n",
    "        f = f1_score((y_test == 'Car').astype(int),(y_hat == 'Car').astype(int))\n",
    "        return len(a[a==True]) / len(y_test),f\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "domestic-carter",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ensemble():\n",
    "    def __init__(self,model_num,accuracy_measure,model_list = None):\n",
    "        self.model_num = model_num\n",
    "        self.accuracy_measure = accuracy_measure\n",
    "        self.model_list = model_list\n",
    "        \n",
    "        \n",
    "    def find_ensemble(self,df_acc,traj_len,vehicle_density,predict_proba = False):\n",
    "        self.is_predict_proba = predict_proba\n",
    "        self.model_list = df_acc.loc[(slice(None),'accuracy','mean'),(vehicle_density,traj_len,self.accuracy_measure)].sort_values(ascending = False).index.get_level_values(0)[:self.model_num].to_list()\n",
    "      \n",
    "    def fit(self,X,y,model_dict=None):\n",
    "        self.model_dict = model_dict\n",
    "        \n",
    "        if model_dict == None:\n",
    "            self.model_dict = {}\n",
    "            for model in self.model_list:\n",
    "                self.model_dict[model] = model.fit(X,y)\n",
    "                \n",
    "        values_view = model_dict.values()\n",
    "        value_iterator = iter(values_view)\n",
    "        self.classes_ = next(value_iterator).classes_  \n",
    "                \n",
    "    \n",
    "    def predict(self,X):\n",
    "        label_list = []\n",
    "        df_model = pd.DataFrame(columns = self.model_list)\n",
    "        \n",
    "        if self.is_predict_proba == False:\n",
    "            for model in self.model_list:\n",
    "                df_model[model] = self.model_dict[model].predict(X)\n",
    "            return df_model.apply(lambda x : x.mode(),axis = 1)[0].to_numpy()\n",
    "            \n",
    "        else:\n",
    "            return self.predict_proba(X,get_label = True)\n",
    "    \n",
    "    def predict_proba(self,X,get_label = False):\n",
    "        label_list = []\n",
    "        model = list(self.model_dict.values())[0]\n",
    "        df_model = pd.DataFrame(columns = pd.MultiIndex.from_product([self.model_list,model.classes_]))#,index = np.arange(0,len(X)))\n",
    "        #df_model.loc[:,('MLP',model.classes_)] =  model.predict_proba(X)\n",
    "        for name in self.model_list:\n",
    "            model = self.model_dict[name]\n",
    "            \n",
    "            df_model.loc[:,(name,model.classes_)] = model.predict_proba(X)\n",
    "            \n",
    "        df_model = df_model.mean(axis=1, level=[1])\n",
    "        \n",
    "        if get_label == True:\n",
    "            return df_model.idxmax(axis=1).to_numpy()\n",
    "        else:\n",
    "            return df_model.to_numpy()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "blessed-nursery",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initial parameters\n",
    "features_to_select = 10\n",
    "models = {\n",
    "        'Random Forest': Pipeline([('scaler', StandardScaler()), ('rf', RandomForestClassifier())]),\n",
    "        'AdaBoost':Pipeline([('scaler', StandardScaler()), ('abc', AdaBoostClassifier())]) ,\n",
    "        'SVM': Pipeline([('scaler', StandardScaler()), ('svc', SVC(max_iter=10000,probability = True))]) ,\n",
    "        'Log Regression': Pipeline([('scaler', StandardScaler()), ('lr', LogisticRegression(max_iter=10000))]) ,\n",
    "        'GBM': Pipeline([('scaler', StandardScaler()), ('gbm', GradientBoostingClassifier())]),\n",
    "        'MLP': Pipeline([('scaler', StandardScaler()), ('mlp', MLPClassifier(hidden_layer_sizes = (250,100,25),max_iter=1000,\\\n",
    "                                                                             learning_rate = 'adaptive',early_stopping = True,n_iter_no_change = 10))])\n",
    "                        \n",
    "    }\n",
    "\n",
    "\n",
    "df_acc = pd.DataFrame(index=pd.MultiIndex.from_product([models.keys(),['f1_score','accuracy'], ['mean']]))\n",
    "overlap = 0.7\n",
    "min_movement_limit = 0.75\n",
    "speed_limit = 0\n",
    "k = 5\n",
    "validation_ratio = 0.2\n",
    "kf = StratifiedKFold(n_splits=k, shuffle=True)\n",
    "accs = np.zeros(k)\n",
    "f1 = np.zeros(k)\n",
    "\n",
    "agg_dict = {'xtrack_diff': ['mean','std'],\n",
    "            'xtrack_dist': ['mean','std'],\n",
    "            'avg_surr_speed': ['mean','std'],\n",
    "            'lanes':['mean'],\n",
    "            'len':['mean'],\n",
    "            'speed':['mean','std'],\n",
    "            #'speed_bool': ['count','sum'],\n",
    "            'acc_edge': ['mean','std'],\n",
    "            'acc_per_edge': ['mean','std']\n",
    "            }\n",
    "            \n",
    "feature_list = ['xtrack_diff','xtrack_dist','avg_surr_speed','speed','acc_edge','acc_per_edge']\n",
    "\n",
    "\n",
    "\n",
    "# factor by which the number of columns will increase after pivoting\n",
    "col_factor = 2\n",
    "\n",
    "# size of aggregating window in seconds\n",
    "window = '5S'\n",
    "\n",
    "\n",
    "# agg_dict = {'xtrack_diff': ['mean','std','skew',pd.DataFrame.kurt],\n",
    "#             'xtrack_dist': ['mean','std','skew',pd.DataFrame.kurt],\n",
    "#             'avg_surr_speed': ['mean','std','skew',pd.DataFrame.kurt],\n",
    "#             'lanes':['mean'],\n",
    "#             'len':['mean'],\n",
    "#             'speed':['mean','std','skew',pd.DataFrame.kurt],\n",
    "#             'acc_edge': ['mean','std','skew',pd.DataFrame.kurt],\n",
    "#             'acc_per_edge': ['mean','std','skew',pd.DataFrame.kurt],\n",
    "#             'xtrack_diff_sq': ['mean','std','skew',pd.DataFrame.kurt],\n",
    "#             'acc_edge_sq': ['mean','std','skew',pd.DataFrame.kurt],\n",
    "#             'acc_per_edge_sq': ['mean','std','skew',pd.DataFrame.kurt],\n",
    "#             'vehicle_density_by_lane':['mean','std','skew',pd.DataFrame.kurt] }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "multiple-literature",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of trajectories:  10261\n",
      "No of Car trajectories:  5136\n",
      "No of Taxi trajectories:  5125\n",
      "No of trajectories:  4481\n",
      "No of Car trajectories:  2246\n",
      "No of Taxi trajectories:  2235\n",
      "No of trajectories:  2672\n",
      "No of Car trajectories:  1336\n",
      "No of Taxi trajectories:  1336\n",
      "No of trajectories:  1877\n",
      "No of Car trajectories:  939\n",
      "No of Taxi trajectories:  938\n",
      "No of trajectories:  1320\n",
      "No of Car trajectories:  661\n",
      "No of Taxi trajectories:  659\n",
      "No of trajectories:  9437\n",
      "No of Car trajectories:  4711\n",
      "No of Car_1 trajectories:  4726\n",
      "No of trajectories:  4249\n",
      "No of Car trajectories:  2119\n",
      "No of Car_1 trajectories:  2130\n",
      "No of trajectories:  2403\n",
      "No of Car trajectories:  1199\n",
      "No of Car_1 trajectories:  1204\n",
      "No of trajectories:  1565\n",
      "No of Car trajectories:  783\n",
      "No of Car_1 trajectories:  782\n",
      "No of trajectories:  1115\n",
      "No of Car trajectories:  557\n",
      "No of Car_1 trajectories:  558\n"
     ]
    }
   ],
   "source": [
    "# Car and Taxi classification\n",
    "#plt.ioff()\n",
    "col_factors = np.arange(10,60, step=10)\n",
    "df_acc = pd.DataFrame(columns = pd.MultiIndex.from_product([[1],col_factors,['test','val_mean','val_log_voting']]),index=pd.MultiIndex.from_product([models.keys(),['accuracy','accuracy_baseline'], ['mean']]))\n",
    "ensemble_2 = ensemble(2,'test')\n",
    "ensemble_3 = ensemble(3,'test')\n",
    "ensemble_5 = ensemble(5,'test')\n",
    "validation_ratio = 0.2\n",
    "test_ratio = 0.2\n",
    "pca = PCA(n_components=5)\n",
    "is_pca = False\n",
    "vehicle_density = 1\n",
    "is_log_model_voting = True\n",
    "feature_generation_method = 'downsample'\n",
    "col_factor = 30\n",
    "window = '0.12S'\n",
    "min_movement_limit = 1\n",
    "for vehicle in ['Taxi','Car_1']:\n",
    "    \n",
    "    if vehicle == 'Car_1':\n",
    "        df_type = df[df.type == 'Car']\n",
    "        accuracy_metric = 'accuracy_baseline'\n",
    "    else : \n",
    "        df_type = df.copy()\n",
    "        accuracy_metric = 'accuracy'\n",
    "        \n",
    "    for col_factor in col_factors:\n",
    "        traj_len = col_factor\n",
    "        df_filtered = df_type.groupby(df_type.index.names[:-1]) \\\n",
    "                .filter(lambda grp: (len(grp) >= col_factor*3)&(speed_ratio(grp,speed_limit) >=min_movement_limit)) \n",
    "        \n",
    "        #df_filtered['speed_bool'] = (df_filtered['speed']>speed_limit).astype(int)\n",
    "        \n",
    "        if vehicle == 'Car_1':\n",
    "            #sample 50% of cars and label them as car_1\n",
    "            df_index = df_filtered.reset_index()[['file_name','id']].drop_duplicates()\n",
    "            df_filtered.loc[df_filtered.reset_index(['edge_id', 'time'],drop = True).index.isin(df_index.sample(frac = 0.5).set_index(['file_name','id']).index),'type']=vehicle\n",
    " \n",
    "        df_train_test,df_val = validation_set(df_filtered,validation_ratio)\n",
    "        df_train,df_test = validation_set(df_train_test,test_ratio)\n",
    "\n",
    "        #aggregate trajectories\n",
    "        #X,y = get_xy(df_train_test,overlap,traj_len,agg_dict,1)\n",
    "        X_train,y_train = get_xy(df_train,outlier_limit = 1,balance = 'by_edge',downsample_feature_list = feature_list,col_factor = col_factor, window =window)\n",
    "        X_test,y_test = get_xy(df_test,balance = 'by_type',downsample_feature_list = feature_list,col_factor =col_factor, window = window)\n",
    "        X_test_voting,y_test_voting = get_xy(df_test,downsample_feature_list = feature_list,col_factor = col_factor, window = window)\n",
    "        X_val,y_val = get_xy(df_val,downsample_feature_list = feature_list,col_factor = col_factor, window = window)\n",
    "\n",
    "        if is_pca:\n",
    "            pca.fit(X_train)\n",
    "            X_test_voting = pd.DataFrame(data = pca.transform(X_test_voting),index = X_test_voting.index)\n",
    "            X_train = pd.DataFrame(data = pca.transform(X_train),index = X_train.index)\n",
    "            X_test = pd.DataFrame(data = pca.transform(X_test),index = X_test.index)\n",
    "            X_val = pd.DataFrame(data = pca.transform(X_val),index = X_val.index)\n",
    "\n",
    "        #store percent cars and taxis\n",
    "        print(\"No of trajectories: \",len(X_train))\n",
    "        print(\"No of Car trajectories: \",sum(y_train == 'Car'))\n",
    "        print(\"No of \"+vehicle+\" trajectories: \",sum(y_train == vehicle))\n",
    "        \n",
    "        df_acc.loc[('traj_len','Car_'+vehicle,'total'), (vehicle_density,traj_len,'test')] = len(X_test)\n",
    "        df_acc.loc[('traj_len','Car_'+vehicle+'_percent','Car'), (vehicle_density,traj_len,'test')] = sum(y_test == 'Car')/len(X_test)\n",
    "        df_acc.loc[('traj_len','Car_'+vehicle+'_percent',vehicle), (vehicle_density,traj_len,'test')] = sum(y_test == vehicle)/ len(X_test)\n",
    "\n",
    "        woedge_count = y_val.reset_index(['edge_id'],drop = True).reset_index().drop_duplicates()\n",
    "        df_acc.loc[('traj_len','Car_'+vehicle,'total'), (vehicle_density,traj_len,'val_mean')] = len(woedge_count)\n",
    "        df_acc.loc[('traj_len','Car_'+vehicle+'_percent','Car'), (vehicle_density,traj_len,'val_mean')] = sum(woedge_count.type == 'Car')/len(woedge_count)\n",
    "        df_acc.loc[('traj_len','Car_'+vehicle+'_percent',vehicle), (vehicle_density,traj_len,'val_mean')] =sum(woedge_count.type == vehicle)/len(woedge_count)\n",
    "\n",
    "        #by_edge_count = y_val.reset_index().drop_duplicates()\n",
    "        df_acc.loc[('traj_len','Car_'+vehicle,'total'), (vehicle_density,traj_len,'val_log_voting')] = len(woedge_count)\n",
    "        df_acc.loc[('traj_len','Car_'+vehicle+'_percent','Car'), (vehicle_density,traj_len,'val_log_voting')] = sum(woedge_count.type == 'Car')/len(woedge_count)\n",
    "        df_acc.loc[('traj_len','Car_'+vehicle+'_percent',vehicle), (vehicle_density,traj_len,'val_log_voting')] = sum(woedge_count.type == vehicle)/len(woedge_count)\n",
    "\n",
    "        model_dict = {}\n",
    "        \n",
    "        # fit different models\n",
    "        for name, model in models.items():\n",
    "\n",
    "            #fit the model on training set\n",
    "            model.fit(X_train,y_train)\n",
    "\n",
    "            #test the model on testing set and save accuracy estimate as test (this accuracy estimate will be used to find ensemble) \n",
    "            val_accs,val_f1 = train_and_accuracy(X_test,y_test,model)                                   \n",
    "            df_acc.loc[(name, accuracy_metric,'mean'),  (vehicle_density,traj_len,'test')] = round(100*val_accs, 3)\n",
    "            \n",
    "            #find accuracy of the model on validation set with voting using mean\n",
    "            val_accs,val_f1 = val_voting_accuracy(X_val,y_val, model,predict_proba = True)\n",
    "            df_acc.loc[(name, accuracy_metric,'mean'), (vehicle_density,traj_len,'val_mean')] = round(100*val_accs, 3)\n",
    "            #plt.savefig(\"traj_len\"+str(traj_len)+name+\".png\")\n",
    "            \n",
    "            if is_log_model_voting:\n",
    "            #train logistic regression for voting using the test set and training model\n",
    "                voting_m = voting_model(model,X_test_voting,y_test_voting)\n",
    "            #find the accuracy of the model on validation set with voting using logistic regression\n",
    "                val_accs,val_f1 = voting_m.accuracy(X_val,y_val)#, voting_m, predict_proba = False)\n",
    "                df_acc.loc[(name, accuracy_metric,'mean'), (vehicle_density,traj_len,'val_log_voting')] = round(100*val_accs, 3)\n",
    "    \n",
    "            #save model in dictionary for ensemble\n",
    "            model_dict[name] = model\n",
    "\n",
    "        #generate ensembles with 2,3 and 5 models\n",
    "        ensemble_2.find_ensemble(df_acc,traj_len,vehicle_density,True)\n",
    "        ensemble_2.fit(X_train,y_train,model_dict)\n",
    "        ensemble_3.find_ensemble(df_acc,traj_len,vehicle_density,True)\n",
    "        ensemble_3.fit(X_train,y_train,model_dict)\n",
    "        ensemble_5.find_ensemble(df_acc,traj_len,vehicle_density,True)\n",
    "        ensemble_5.fit(X_train,y_train,model_dict)\n",
    "\n",
    "        #test accuracy of ensembles on validation set with mean \n",
    "        val_accs,val_f1 = val_voting_accuracy(X_val,y_val, ensemble_2)\n",
    "        df_acc.loc[('ensemble_2', accuracy_metric,'mean'), (vehicle_density,traj_len,'val_mean')] = round(100*val_accs, 3)\n",
    "        val_accs,val_f1 = val_voting_accuracy(X_val,y_val, ensemble_3)\n",
    "        df_acc.loc[('ensemble_3', accuracy_metric,'mean'), (vehicle_density,traj_len,'val_mean')] = round(100*val_accs, 3)\n",
    "        val_accs,val_f1 = val_voting_accuracy(X_val,y_val, ensemble_5)\n",
    "        df_acc.loc[('ensemble_5', accuracy_metric,'mean'), (vehicle_density,traj_len,'val_mean')] = round(100*val_accs, 3)\n",
    "\n",
    "        if is_log_model_voting:\n",
    "            #test accuracy of ensembles on validation using logistic voting (trained on testing set)\n",
    "            voting_m = voting_model(ensemble_2,X_test_voting,y_test_voting)\n",
    "            val_accs,val_f1 = voting_m.accuracy(X_val,y_val)\n",
    "            df_acc.loc[('ensemble_2', accuracy_metric,'mean'), (vehicle_density,traj_len,'val_log_voting')] = round(100*val_accs, 3)\n",
    "            voting_m = voting_model(ensemble_3,X_test_voting,y_test_voting)\n",
    "            val_accs,val_f1 = voting_m.accuracy(X_val,y_val)\n",
    "            df_acc.loc[('ensemble_3', accuracy_metric,'mean'), (vehicle_density,traj_len,'val_log_voting')] = round(100*val_accs, 3)\n",
    "            voting_m = voting_model(ensemble_5,X_test_voting,y_test_voting)\n",
    "            val_accs,val_f1 = voting_m.accuracy(X_val,y_val)\n",
    "            df_acc.loc[('ensemble_5', accuracy_metric,'mean'), (vehicle_density,traj_len,'val_log_voting')] = round(100*val_accs, 3)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "cognitive-scroll",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"15\" halign=\"left\">1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">10</th>\n",
       "      <th colspan=\"3\" halign=\"left\">20</th>\n",
       "      <th colspan=\"3\" halign=\"left\">30</th>\n",
       "      <th colspan=\"3\" halign=\"left\">40</th>\n",
       "      <th colspan=\"3\" halign=\"left\">50</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>test</th>\n",
       "      <th>val_mean</th>\n",
       "      <th>val_log_voting</th>\n",
       "      <th>test</th>\n",
       "      <th>val_mean</th>\n",
       "      <th>val_log_voting</th>\n",
       "      <th>test</th>\n",
       "      <th>val_mean</th>\n",
       "      <th>val_log_voting</th>\n",
       "      <th>test</th>\n",
       "      <th>val_mean</th>\n",
       "      <th>val_log_voting</th>\n",
       "      <th>test</th>\n",
       "      <th>val_mean</th>\n",
       "      <th>val_log_voting</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">AdaBoost</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>mean</th>\n",
       "      <td>53.737</td>\n",
       "      <td>55.2</td>\n",
       "      <td>52.0</td>\n",
       "      <td>54.758</td>\n",
       "      <td>59.426</td>\n",
       "      <td>55.738</td>\n",
       "      <td>52.524</td>\n",
       "      <td>56.034</td>\n",
       "      <td>56.466</td>\n",
       "      <td>51.976</td>\n",
       "      <td>56.364</td>\n",
       "      <td>55.455</td>\n",
       "      <td>52.273</td>\n",
       "      <td>52.381</td>\n",
       "      <td>50.476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy_baseline</th>\n",
       "      <th>mean</th>\n",
       "      <td>49.339</td>\n",
       "      <td>51.014</td>\n",
       "      <td>48.311</td>\n",
       "      <td>48.918</td>\n",
       "      <td>47.902</td>\n",
       "      <td>43.007</td>\n",
       "      <td>52.273</td>\n",
       "      <td>48.872</td>\n",
       "      <td>47.368</td>\n",
       "      <td>50.526</td>\n",
       "      <td>52.033</td>\n",
       "      <td>45.935</td>\n",
       "      <td>54.762</td>\n",
       "      <td>46.087</td>\n",
       "      <td>50.435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">GBM</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>mean</th>\n",
       "      <td>59.942</td>\n",
       "      <td>60.0</td>\n",
       "      <td>63.2</td>\n",
       "      <td>55.514</td>\n",
       "      <td>58.607</td>\n",
       "      <td>59.836</td>\n",
       "      <td>57.571</td>\n",
       "      <td>61.207</td>\n",
       "      <td>60.776</td>\n",
       "      <td>56.917</td>\n",
       "      <td>57.273</td>\n",
       "      <td>53.636</td>\n",
       "      <td>59.091</td>\n",
       "      <td>58.095</td>\n",
       "      <td>48.571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy_baseline</th>\n",
       "      <th>mean</th>\n",
       "      <td>49.617</td>\n",
       "      <td>50.676</td>\n",
       "      <td>48.311</td>\n",
       "      <td>48.532</td>\n",
       "      <td>50.699</td>\n",
       "      <td>48.951</td>\n",
       "      <td>51.136</td>\n",
       "      <td>47.744</td>\n",
       "      <td>51.504</td>\n",
       "      <td>50.175</td>\n",
       "      <td>50.407</td>\n",
       "      <td>51.626</td>\n",
       "      <td>52.381</td>\n",
       "      <td>45.652</td>\n",
       "      <td>52.174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Log Regression</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>mean</th>\n",
       "      <td>50.581</td>\n",
       "      <td>50.4</td>\n",
       "      <td>52.8</td>\n",
       "      <td>50.982</td>\n",
       "      <td>53.689</td>\n",
       "      <td>52.869</td>\n",
       "      <td>52.681</td>\n",
       "      <td>49.569</td>\n",
       "      <td>51.724</td>\n",
       "      <td>51.383</td>\n",
       "      <td>51.818</td>\n",
       "      <td>54.091</td>\n",
       "      <td>53.788</td>\n",
       "      <td>53.333</td>\n",
       "      <td>53.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy_baseline</th>\n",
       "      <th>mean</th>\n",
       "      <td>50.104</td>\n",
       "      <td>50.676</td>\n",
       "      <td>46.284</td>\n",
       "      <td>52.628</td>\n",
       "      <td>48.601</td>\n",
       "      <td>45.455</td>\n",
       "      <td>51.136</td>\n",
       "      <td>50.752</td>\n",
       "      <td>48.872</td>\n",
       "      <td>47.895</td>\n",
       "      <td>52.846</td>\n",
       "      <td>50.813</td>\n",
       "      <td>51.667</td>\n",
       "      <td>49.565</td>\n",
       "      <td>46.522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">MLP</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>mean</th>\n",
       "      <td>56.495</td>\n",
       "      <td>58.8</td>\n",
       "      <td>58.8</td>\n",
       "      <td>55.891</td>\n",
       "      <td>60.246</td>\n",
       "      <td>61.885</td>\n",
       "      <td>54.732</td>\n",
       "      <td>61.207</td>\n",
       "      <td>58.621</td>\n",
       "      <td>55.731</td>\n",
       "      <td>56.364</td>\n",
       "      <td>62.273</td>\n",
       "      <td>54.04</td>\n",
       "      <td>54.762</td>\n",
       "      <td>55.714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy_baseline</th>\n",
       "      <th>mean</th>\n",
       "      <td>50.0</td>\n",
       "      <td>50.338</td>\n",
       "      <td>51.689</td>\n",
       "      <td>51.468</td>\n",
       "      <td>53.497</td>\n",
       "      <td>53.846</td>\n",
       "      <td>51.515</td>\n",
       "      <td>51.88</td>\n",
       "      <td>43.233</td>\n",
       "      <td>49.298</td>\n",
       "      <td>53.252</td>\n",
       "      <td>46.748</td>\n",
       "      <td>43.81</td>\n",
       "      <td>50.0</td>\n",
       "      <td>51.739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Random Forest</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>mean</th>\n",
       "      <td>57.293</td>\n",
       "      <td>58.8</td>\n",
       "      <td>56.0</td>\n",
       "      <td>56.269</td>\n",
       "      <td>58.197</td>\n",
       "      <td>58.197</td>\n",
       "      <td>55.994</td>\n",
       "      <td>60.345</td>\n",
       "      <td>56.466</td>\n",
       "      <td>56.126</td>\n",
       "      <td>64.545</td>\n",
       "      <td>59.091</td>\n",
       "      <td>56.566</td>\n",
       "      <td>59.048</td>\n",
       "      <td>58.095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy_baseline</th>\n",
       "      <th>mean</th>\n",
       "      <td>49.826</td>\n",
       "      <td>47.635</td>\n",
       "      <td>44.932</td>\n",
       "      <td>51.7</td>\n",
       "      <td>51.399</td>\n",
       "      <td>46.503</td>\n",
       "      <td>52.525</td>\n",
       "      <td>49.624</td>\n",
       "      <td>52.256</td>\n",
       "      <td>49.298</td>\n",
       "      <td>45.935</td>\n",
       "      <td>54.065</td>\n",
       "      <td>50.714</td>\n",
       "      <td>44.783</td>\n",
       "      <td>46.522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">SVM</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>mean</th>\n",
       "      <td>56.168</td>\n",
       "      <td>61.2</td>\n",
       "      <td>61.6</td>\n",
       "      <td>53.927</td>\n",
       "      <td>62.705</td>\n",
       "      <td>62.295</td>\n",
       "      <td>55.363</td>\n",
       "      <td>62.069</td>\n",
       "      <td>57.328</td>\n",
       "      <td>57.115</td>\n",
       "      <td>64.545</td>\n",
       "      <td>63.182</td>\n",
       "      <td>57.828</td>\n",
       "      <td>58.571</td>\n",
       "      <td>54.762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy_baseline</th>\n",
       "      <th>mean</th>\n",
       "      <td>49.93</td>\n",
       "      <td>52.703</td>\n",
       "      <td>50.0</td>\n",
       "      <td>51.7</td>\n",
       "      <td>54.196</td>\n",
       "      <td>53.147</td>\n",
       "      <td>47.98</td>\n",
       "      <td>51.128</td>\n",
       "      <td>48.872</td>\n",
       "      <td>51.579</td>\n",
       "      <td>49.593</td>\n",
       "      <td>47.967</td>\n",
       "      <td>49.286</td>\n",
       "      <td>46.957</td>\n",
       "      <td>47.826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">ensemble_2</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>60.8</td>\n",
       "      <td>60.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>56.967</td>\n",
       "      <td>63.115</td>\n",
       "      <td>NaN</td>\n",
       "      <td>59.483</td>\n",
       "      <td>61.638</td>\n",
       "      <td>NaN</td>\n",
       "      <td>62.727</td>\n",
       "      <td>61.364</td>\n",
       "      <td>NaN</td>\n",
       "      <td>58.571</td>\n",
       "      <td>57.143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy_baseline</th>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>51.014</td>\n",
       "      <td>48.649</td>\n",
       "      <td>NaN</td>\n",
       "      <td>53.497</td>\n",
       "      <td>51.399</td>\n",
       "      <td>NaN</td>\n",
       "      <td>46.241</td>\n",
       "      <td>49.248</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50.0</td>\n",
       "      <td>53.252</td>\n",
       "      <td>NaN</td>\n",
       "      <td>48.261</td>\n",
       "      <td>50.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">ensemble_3</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>60.0</td>\n",
       "      <td>56.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>59.836</td>\n",
       "      <td>61.885</td>\n",
       "      <td>NaN</td>\n",
       "      <td>62.069</td>\n",
       "      <td>62.069</td>\n",
       "      <td>NaN</td>\n",
       "      <td>63.636</td>\n",
       "      <td>61.818</td>\n",
       "      <td>NaN</td>\n",
       "      <td>59.048</td>\n",
       "      <td>59.048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy_baseline</th>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>53.041</td>\n",
       "      <td>50.338</td>\n",
       "      <td>NaN</td>\n",
       "      <td>55.944</td>\n",
       "      <td>52.448</td>\n",
       "      <td>NaN</td>\n",
       "      <td>47.744</td>\n",
       "      <td>50.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>54.065</td>\n",
       "      <td>52.033</td>\n",
       "      <td>NaN</td>\n",
       "      <td>48.261</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">ensemble_5</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>58.8</td>\n",
       "      <td>57.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>60.246</td>\n",
       "      <td>63.934</td>\n",
       "      <td>NaN</td>\n",
       "      <td>61.207</td>\n",
       "      <td>61.207</td>\n",
       "      <td>NaN</td>\n",
       "      <td>60.909</td>\n",
       "      <td>64.545</td>\n",
       "      <td>NaN</td>\n",
       "      <td>58.095</td>\n",
       "      <td>56.667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy_baseline</th>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>54.392</td>\n",
       "      <td>51.014</td>\n",
       "      <td>NaN</td>\n",
       "      <td>57.692</td>\n",
       "      <td>52.448</td>\n",
       "      <td>NaN</td>\n",
       "      <td>53.759</td>\n",
       "      <td>48.872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>53.659</td>\n",
       "      <td>49.593</td>\n",
       "      <td>NaN</td>\n",
       "      <td>48.696</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">traj_len</th>\n",
       "      <th>Car_Car_1</th>\n",
       "      <th>total</th>\n",
       "      <td>2874</td>\n",
       "      <td>296</td>\n",
       "      <td>296</td>\n",
       "      <td>1294</td>\n",
       "      <td>286</td>\n",
       "      <td>286</td>\n",
       "      <td>792</td>\n",
       "      <td>266</td>\n",
       "      <td>266</td>\n",
       "      <td>570</td>\n",
       "      <td>246</td>\n",
       "      <td>246</td>\n",
       "      <td>420</td>\n",
       "      <td>230</td>\n",
       "      <td>230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Car_Car_1_percent</th>\n",
       "      <th>Car</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Car_1</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Car_Taxi</th>\n",
       "      <th>total</th>\n",
       "      <td>2756</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "      <td>1324</td>\n",
       "      <td>244</td>\n",
       "      <td>244</td>\n",
       "      <td>634</td>\n",
       "      <td>232</td>\n",
       "      <td>232</td>\n",
       "      <td>506</td>\n",
       "      <td>220</td>\n",
       "      <td>220</td>\n",
       "      <td>396</td>\n",
       "      <td>210</td>\n",
       "      <td>210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Car_Taxi_percent</th>\n",
       "      <th>Car</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Taxi</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             1                          \\\n",
       "                                            10                           \n",
       "                                          test val_mean val_log_voting   \n",
       "AdaBoost       accuracy          mean   53.737     55.2           52.0   \n",
       "               accuracy_baseline mean   49.339   51.014         48.311   \n",
       "GBM            accuracy          mean   59.942     60.0           63.2   \n",
       "               accuracy_baseline mean   49.617   50.676         48.311   \n",
       "Log Regression accuracy          mean   50.581     50.4           52.8   \n",
       "               accuracy_baseline mean   50.104   50.676         46.284   \n",
       "MLP            accuracy          mean   56.495     58.8           58.8   \n",
       "               accuracy_baseline mean     50.0   50.338         51.689   \n",
       "Random Forest  accuracy          mean   57.293     58.8           56.0   \n",
       "               accuracy_baseline mean   49.826   47.635         44.932   \n",
       "SVM            accuracy          mean   56.168     61.2           61.6   \n",
       "               accuracy_baseline mean    49.93   52.703           50.0   \n",
       "ensemble_2     accuracy          mean      NaN     60.8           60.4   \n",
       "               accuracy_baseline mean      NaN   51.014         48.649   \n",
       "ensemble_3     accuracy          mean      NaN     60.0           56.4   \n",
       "               accuracy_baseline mean      NaN   53.041         50.338   \n",
       "ensemble_5     accuracy          mean      NaN     58.8           57.2   \n",
       "               accuracy_baseline mean      NaN   54.392         51.014   \n",
       "traj_len       Car_Car_1         total    2874      296            296   \n",
       "               Car_Car_1_percent Car       0.5      0.5            0.5   \n",
       "                                 Car_1     0.5      0.5            0.5   \n",
       "               Car_Taxi          total    2756      250            250   \n",
       "               Car_Taxi_percent  Car       0.5      0.5            0.5   \n",
       "                                 Taxi      0.5      0.5            0.5   \n",
       "\n",
       "                                                                        \\\n",
       "                                            20                           \n",
       "                                          test val_mean val_log_voting   \n",
       "AdaBoost       accuracy          mean   54.758   59.426         55.738   \n",
       "               accuracy_baseline mean   48.918   47.902         43.007   \n",
       "GBM            accuracy          mean   55.514   58.607         59.836   \n",
       "               accuracy_baseline mean   48.532   50.699         48.951   \n",
       "Log Regression accuracy          mean   50.982   53.689         52.869   \n",
       "               accuracy_baseline mean   52.628   48.601         45.455   \n",
       "MLP            accuracy          mean   55.891   60.246         61.885   \n",
       "               accuracy_baseline mean   51.468   53.497         53.846   \n",
       "Random Forest  accuracy          mean   56.269   58.197         58.197   \n",
       "               accuracy_baseline mean     51.7   51.399         46.503   \n",
       "SVM            accuracy          mean   53.927   62.705         62.295   \n",
       "               accuracy_baseline mean     51.7   54.196         53.147   \n",
       "ensemble_2     accuracy          mean      NaN   56.967         63.115   \n",
       "               accuracy_baseline mean      NaN   53.497         51.399   \n",
       "ensemble_3     accuracy          mean      NaN   59.836         61.885   \n",
       "               accuracy_baseline mean      NaN   55.944         52.448   \n",
       "ensemble_5     accuracy          mean      NaN   60.246         63.934   \n",
       "               accuracy_baseline mean      NaN   57.692         52.448   \n",
       "traj_len       Car_Car_1         total    1294      286            286   \n",
       "               Car_Car_1_percent Car       0.5      0.5            0.5   \n",
       "                                 Car_1     0.5      0.5            0.5   \n",
       "               Car_Taxi          total    1324      244            244   \n",
       "               Car_Taxi_percent  Car       0.5      0.5            0.5   \n",
       "                                 Taxi      0.5      0.5            0.5   \n",
       "\n",
       "                                                                        \\\n",
       "                                            30                           \n",
       "                                          test val_mean val_log_voting   \n",
       "AdaBoost       accuracy          mean   52.524   56.034         56.466   \n",
       "               accuracy_baseline mean   52.273   48.872         47.368   \n",
       "GBM            accuracy          mean   57.571   61.207         60.776   \n",
       "               accuracy_baseline mean   51.136   47.744         51.504   \n",
       "Log Regression accuracy          mean   52.681   49.569         51.724   \n",
       "               accuracy_baseline mean   51.136   50.752         48.872   \n",
       "MLP            accuracy          mean   54.732   61.207         58.621   \n",
       "               accuracy_baseline mean   51.515    51.88         43.233   \n",
       "Random Forest  accuracy          mean   55.994   60.345         56.466   \n",
       "               accuracy_baseline mean   52.525   49.624         52.256   \n",
       "SVM            accuracy          mean   55.363   62.069         57.328   \n",
       "               accuracy_baseline mean    47.98   51.128         48.872   \n",
       "ensemble_2     accuracy          mean      NaN   59.483         61.638   \n",
       "               accuracy_baseline mean      NaN   46.241         49.248   \n",
       "ensemble_3     accuracy          mean      NaN   62.069         62.069   \n",
       "               accuracy_baseline mean      NaN   47.744           50.0   \n",
       "ensemble_5     accuracy          mean      NaN   61.207         61.207   \n",
       "               accuracy_baseline mean      NaN   53.759         48.872   \n",
       "traj_len       Car_Car_1         total     792      266            266   \n",
       "               Car_Car_1_percent Car       0.5      0.5            0.5   \n",
       "                                 Car_1     0.5      0.5            0.5   \n",
       "               Car_Taxi          total     634      232            232   \n",
       "               Car_Taxi_percent  Car       0.5      0.5            0.5   \n",
       "                                 Taxi      0.5      0.5            0.5   \n",
       "\n",
       "                                                                        \\\n",
       "                                            40                           \n",
       "                                          test val_mean val_log_voting   \n",
       "AdaBoost       accuracy          mean   51.976   56.364         55.455   \n",
       "               accuracy_baseline mean   50.526   52.033         45.935   \n",
       "GBM            accuracy          mean   56.917   57.273         53.636   \n",
       "               accuracy_baseline mean   50.175   50.407         51.626   \n",
       "Log Regression accuracy          mean   51.383   51.818         54.091   \n",
       "               accuracy_baseline mean   47.895   52.846         50.813   \n",
       "MLP            accuracy          mean   55.731   56.364         62.273   \n",
       "               accuracy_baseline mean   49.298   53.252         46.748   \n",
       "Random Forest  accuracy          mean   56.126   64.545         59.091   \n",
       "               accuracy_baseline mean   49.298   45.935         54.065   \n",
       "SVM            accuracy          mean   57.115   64.545         63.182   \n",
       "               accuracy_baseline mean   51.579   49.593         47.967   \n",
       "ensemble_2     accuracy          mean      NaN   62.727         61.364   \n",
       "               accuracy_baseline mean      NaN     50.0         53.252   \n",
       "ensemble_3     accuracy          mean      NaN   63.636         61.818   \n",
       "               accuracy_baseline mean      NaN   54.065         52.033   \n",
       "ensemble_5     accuracy          mean      NaN   60.909         64.545   \n",
       "               accuracy_baseline mean      NaN   53.659         49.593   \n",
       "traj_len       Car_Car_1         total     570      246            246   \n",
       "               Car_Car_1_percent Car       0.5      0.5            0.5   \n",
       "                                 Car_1     0.5      0.5            0.5   \n",
       "               Car_Taxi          total     506      220            220   \n",
       "               Car_Taxi_percent  Car       0.5      0.5            0.5   \n",
       "                                 Taxi      0.5      0.5            0.5   \n",
       "\n",
       "                                                                        \n",
       "                                            50                          \n",
       "                                          test val_mean val_log_voting  \n",
       "AdaBoost       accuracy          mean   52.273   52.381         50.476  \n",
       "               accuracy_baseline mean   54.762   46.087         50.435  \n",
       "GBM            accuracy          mean   59.091   58.095         48.571  \n",
       "               accuracy_baseline mean   52.381   45.652         52.174  \n",
       "Log Regression accuracy          mean   53.788   53.333          53.81  \n",
       "               accuracy_baseline mean   51.667   49.565         46.522  \n",
       "MLP            accuracy          mean    54.04   54.762         55.714  \n",
       "               accuracy_baseline mean    43.81     50.0         51.739  \n",
       "Random Forest  accuracy          mean   56.566   59.048         58.095  \n",
       "               accuracy_baseline mean   50.714   44.783         46.522  \n",
       "SVM            accuracy          mean   57.828   58.571         54.762  \n",
       "               accuracy_baseline mean   49.286   46.957         47.826  \n",
       "ensemble_2     accuracy          mean      NaN   58.571         57.143  \n",
       "               accuracy_baseline mean      NaN   48.261          50.87  \n",
       "ensemble_3     accuracy          mean      NaN   59.048         59.048  \n",
       "               accuracy_baseline mean      NaN   48.261           50.0  \n",
       "ensemble_5     accuracy          mean      NaN   58.095         56.667  \n",
       "               accuracy_baseline mean      NaN   48.696           50.0  \n",
       "traj_len       Car_Car_1         total     420      230            230  \n",
       "               Car_Car_1_percent Car       0.5      0.5            0.5  \n",
       "                                 Car_1     0.5      0.5            0.5  \n",
       "               Car_Taxi          total     396      210            210  \n",
       "               Car_Taxi_percent  Car       0.5      0.5            0.5  \n",
       "                                 Taxi      0.5      0.5            0.5  "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_acc.sort_index()#.to_csv(\"accuracy_block4_downsample.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceramic-alaska",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
