{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "early-disability",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "working-broadway",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "classical_models.ipynb\n",
    "\n",
    "by: Archie Gertsman (arkadiy2@illinois.edu)\n",
    "Lloyd Fernandes (lloydf2@illinois.edu)\n",
    "\n",
    "Project director: Richard Sowers\n",
    "\n",
    "r-sowers@illinois.eduhttps://publish.illinois.edu/r-sowers/\n",
    "\n",
    "Copyright 2019 University of Illinois Board of Trustees. All Rights Reserved. Licensed under the MIT license\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "backed-exclusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings; warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "durable-capability",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../Lib/')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from feature_eng import split_trajectories\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from time import time\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from IPython.display import display\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import RFE\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "finite-dominant",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>speed</th>\n",
       "      <th>lon_acc</th>\n",
       "      <th>lat_acc</th>\n",
       "      <th>type</th>\n",
       "      <th>traveled_d</th>\n",
       "      <th>avg_speed</th>\n",
       "      <th>bearing</th>\n",
       "      <th>nearest_edge_start_node</th>\n",
       "      <th>...</th>\n",
       "      <th>edge_progress_intervals</th>\n",
       "      <th>len</th>\n",
       "      <th>lanes</th>\n",
       "      <th>node_veh_dist</th>\n",
       "      <th>edge_seg</th>\n",
       "      <th>vehicle_density</th>\n",
       "      <th>avg_surr_speed</th>\n",
       "      <th>edge_bearing</th>\n",
       "      <th>acc_edge</th>\n",
       "      <th>acc_per_edge</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>file_name</th>\n",
       "      <th>id</th>\n",
       "      <th>edge_id</th>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">4_1</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">1</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">250699362_250699984</th>\n",
       "      <th>42.00</th>\n",
       "      <td>37.982746</td>\n",
       "      <td>23.732961</td>\n",
       "      <td>11.9046</td>\n",
       "      <td>-0.1145</td>\n",
       "      <td>0.0138</td>\n",
       "      <td>Taxi</td>\n",
       "      <td>182.37</td>\n",
       "      <td>9.740748</td>\n",
       "      <td>1.570795</td>\n",
       "      <td>250699362</td>\n",
       "      <td>...</td>\n",
       "      <td>0.3</td>\n",
       "      <td>97.581</td>\n",
       "      <td>5.4</td>\n",
       "      <td>29.814330</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>10.464171</td>\n",
       "      <td>-2.83013</td>\n",
       "      <td>0.113220</td>\n",
       "      <td>0.021953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42.04</th>\n",
       "      <td>37.982746</td>\n",
       "      <td>23.732963</td>\n",
       "      <td>11.8975</td>\n",
       "      <td>-0.1007</td>\n",
       "      <td>0.0147</td>\n",
       "      <td>Taxi</td>\n",
       "      <td>182.37</td>\n",
       "      <td>9.740748</td>\n",
       "      <td>0.168572</td>\n",
       "      <td>250699362</td>\n",
       "      <td>...</td>\n",
       "      <td>0.3</td>\n",
       "      <td>97.581</td>\n",
       "      <td>5.4</td>\n",
       "      <td>29.674830</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>10.457843</td>\n",
       "      <td>-2.83013</td>\n",
       "      <td>0.100360</td>\n",
       "      <td>0.016867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42.08</th>\n",
       "      <td>37.982747</td>\n",
       "      <td>23.732964</td>\n",
       "      <td>11.8919</td>\n",
       "      <td>-0.0918</td>\n",
       "      <td>0.0157</td>\n",
       "      <td>Taxi</td>\n",
       "      <td>182.37</td>\n",
       "      <td>9.740748</td>\n",
       "      <td>0.168573</td>\n",
       "      <td>250699362</td>\n",
       "      <td>...</td>\n",
       "      <td>0.3</td>\n",
       "      <td>97.581</td>\n",
       "      <td>5.4</td>\n",
       "      <td>29.537753</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>10.452857</td>\n",
       "      <td>-2.83013</td>\n",
       "      <td>0.092194</td>\n",
       "      <td>0.013188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42.12</th>\n",
       "      <td>37.982748</td>\n",
       "      <td>23.732965</td>\n",
       "      <td>11.8871</td>\n",
       "      <td>-0.0869</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>Taxi</td>\n",
       "      <td>182.37</td>\n",
       "      <td>9.740748</td>\n",
       "      <td>1.570796</td>\n",
       "      <td>250699362</td>\n",
       "      <td>...</td>\n",
       "      <td>0.3</td>\n",
       "      <td>97.581</td>\n",
       "      <td>5.4</td>\n",
       "      <td>29.400718</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>10.448586</td>\n",
       "      <td>-2.83013</td>\n",
       "      <td>0.087837</td>\n",
       "      <td>0.010734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42.16</th>\n",
       "      <td>37.982748</td>\n",
       "      <td>23.732966</td>\n",
       "      <td>11.8831</td>\n",
       "      <td>-0.0784</td>\n",
       "      <td>0.0176</td>\n",
       "      <td>Taxi</td>\n",
       "      <td>182.37</td>\n",
       "      <td>9.740748</td>\n",
       "      <td>0.328080</td>\n",
       "      <td>250699362</td>\n",
       "      <td>...</td>\n",
       "      <td>0.3</td>\n",
       "      <td>97.581</td>\n",
       "      <td>5.4</td>\n",
       "      <td>29.330986</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>10.444986</td>\n",
       "      <td>-2.83013</td>\n",
       "      <td>0.080021</td>\n",
       "      <td>0.007273</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              lat        lon    speed  \\\n",
       "file_name id edge_id             time                                   \n",
       "4_1       1  250699362_250699984 42.00  37.982746  23.732961  11.9046   \n",
       "                                 42.04  37.982746  23.732963  11.8975   \n",
       "                                 42.08  37.982747  23.732964  11.8919   \n",
       "                                 42.12  37.982748  23.732965  11.8871   \n",
       "                                 42.16  37.982748  23.732966  11.8831   \n",
       "\n",
       "                                        lon_acc  lat_acc  type  traveled_d  \\\n",
       "file_name id edge_id             time                                        \n",
       "4_1       1  250699362_250699984 42.00  -0.1145   0.0138  Taxi      182.37   \n",
       "                                 42.04  -0.1007   0.0147  Taxi      182.37   \n",
       "                                 42.08  -0.0918   0.0157  Taxi      182.37   \n",
       "                                 42.12  -0.0869   0.0167  Taxi      182.37   \n",
       "                                 42.16  -0.0784   0.0176  Taxi      182.37   \n",
       "\n",
       "                                        avg_speed   bearing  \\\n",
       "file_name id edge_id             time                         \n",
       "4_1       1  250699362_250699984 42.00   9.740748  1.570795   \n",
       "                                 42.04   9.740748  0.168572   \n",
       "                                 42.08   9.740748  0.168573   \n",
       "                                 42.12   9.740748  1.570796   \n",
       "                                 42.16   9.740748  0.328080   \n",
       "\n",
       "                                        nearest_edge_start_node  ...  \\\n",
       "file_name id edge_id             time                            ...   \n",
       "4_1       1  250699362_250699984 42.00                250699362  ...   \n",
       "                                 42.04                250699362  ...   \n",
       "                                 42.08                250699362  ...   \n",
       "                                 42.12                250699362  ...   \n",
       "                                 42.16                250699362  ...   \n",
       "\n",
       "                                        edge_progress_intervals     len  \\\n",
       "file_name id edge_id             time                                     \n",
       "4_1       1  250699362_250699984 42.00                      0.3  97.581   \n",
       "                                 42.04                      0.3  97.581   \n",
       "                                 42.08                      0.3  97.581   \n",
       "                                 42.12                      0.3  97.581   \n",
       "                                 42.16                      0.3  97.581   \n",
       "\n",
       "                                        lanes  node_veh_dist  edge_seg  \\\n",
       "file_name id edge_id             time                                    \n",
       "4_1       1  250699362_250699984 42.00    5.4      29.814330       1.0   \n",
       "                                 42.04    5.4      29.674830       1.0   \n",
       "                                 42.08    5.4      29.537753       1.0   \n",
       "                                 42.12    5.4      29.400718       1.0   \n",
       "                                 42.16    5.4      29.330986       1.0   \n",
       "\n",
       "                                        vehicle_density  avg_surr_speed  \\\n",
       "file_name id edge_id             time                                     \n",
       "4_1       1  250699362_250699984 42.00                7       10.464171   \n",
       "                                 42.04                7       10.457843   \n",
       "                                 42.08                7       10.452857   \n",
       "                                 42.12                7       10.448586   \n",
       "                                 42.16                7       10.444986   \n",
       "\n",
       "                                        edge_bearing  acc_edge  acc_per_edge  \n",
       "file_name id edge_id             time                                         \n",
       "4_1       1  250699362_250699984 42.00      -2.83013  0.113220      0.021953  \n",
       "                                 42.04      -2.83013  0.100360      0.016867  \n",
       "                                 42.08      -2.83013  0.092194      0.013188  \n",
       "                                 42.12      -2.83013  0.087837      0.010734  \n",
       "                                 42.16      -2.83013  0.080021      0.007273  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle('block4_concat_lane.pkl')  \\\n",
    "    .set_index('edge_id', append=True) \\\n",
    "    .reorder_levels((0,1,3,2))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "suspended-little",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_agg(df, agg_dict, window_size=100, step=25):\n",
    "    # rolling agg with step size = 1\n",
    "    df_agg = df.groupby(df.index.names[:-1]) \\\n",
    "                .rolling(window_size) \\\n",
    "                .agg(agg_dict) \\\n",
    "                .dropna()\n",
    "    #print(df_agg)\n",
    "    # select a subset of above computations to achieve custom step size\n",
    "    df_agg = df_agg.groupby(df_agg.index.names, \n",
    "                            as_index=False, \n",
    "                            group_keys=False) \\\n",
    "                .apply(lambda x: x[::step])\n",
    "    #print(df_agg)\n",
    "    df_agg.columns = ['_'.join(col) for col in df_agg.columns]\n",
    "    \n",
    "    \n",
    "    # add 'type' column\n",
    "    vehicle_types = df.type.groupby(df.index.names[:-1]).first()\n",
    "    #print(vehicle_types)\n",
    "    return df_agg.join(vehicle_types)\n",
    "  \n",
    "def speed_ratio(grp, min_speed=0):\n",
    "   \n",
    "    return len(grp[grp.speed > min_speed]) / len(grp)\n",
    "\n",
    "def validation_set(df,test_size):\n",
    "    \"\"\"dataframe is split based on their vehicle id's\"\"\"\n",
    "    df_val = df.reset_index()[[\"file_name\",'id','type']].drop_duplicates()\n",
    "    X,y = df_val[[\"file_name\",\"id\"]],df_val['type']\n",
    "    X_train,X_test,_,y_test = train_test_split(X, y, test_size=test_size, random_state=4, stratify=y) \n",
    "    df_train = df[df.index.droplevel(['time','edge_id']).isin(X_train.set_index(['file_name','id']).index)]\n",
    "    X_test['type'] = y_test\n",
    "    g = X_test.groupby('type')\n",
    "    X_test = g.apply(lambda group: group.sample(g.size().min())).reset_index(drop = True)\n",
    "    df_test = df[df.index.droplevel(['time','edge_id']).isin(X_test.set_index(['file_name','id']).index)]\n",
    "    return df_train,df_test\n",
    "\n",
    "def train_and_accuracy(X_test,y_test, model):\n",
    "\n",
    "    #model.fit(X_train, y_train)\n",
    "    y_hat = model.predict(X_test)\n",
    "    a = y_hat==y_test\n",
    "    \n",
    "    f = f1_score((y_test == 'Car').astype(int),(y_hat == 'Car').astype(int))\n",
    "    return len(a[a==True]) / len(y_test),f\n",
    "\n",
    "def val_voting_accuracy(X_val,y_val, model,by_edge = False,predict_proba = False,display = False):\n",
    "\n",
    "    if predict_proba == True:\n",
    "        \n",
    "        y_hat = pd.DataFrame(index = y_val.index,data = model.predict_proba(X_val),columns = model.classes_)\n",
    "        y_hat_orig = y_hat.copy()\n",
    "        #print(y_hat_orig)\n",
    "        if by_edge == False:\n",
    "            \n",
    "            #predicted value for the entire trajectory would be the mode of the predicted labels\n",
    "            y_hat = y_hat.groupby(['file_name','id']).mean()\n",
    "            y_hat = y_hat.idxmax(axis=1)#.to_numpy()\n",
    "            y_test = y_val.groupby(['file_name','id']).first(['type'])\n",
    "             \n",
    "        else:\n",
    "\n",
    "            #predicted value for the entire trajectory would be the mode of the predicted labels\n",
    "            y_hat = y_hat.groupby(['file_name','id','edge_id']).mean()\n",
    "            y_hat = y_hat.idxmax(axis=1)#.to_numpy()\n",
    "            y_test = y_val.groupby(['file_name','id','edge_id']).first(['type'])\n",
    "\n",
    "    else:\n",
    "        y_hat = model.predict(X_val)\n",
    "        y_hat = pd.DataFrame(index = y_val.index,data = y_hat,columns = ['type'])\n",
    "        y_hat_orig = y_hat.copy()\n",
    "        if by_edge == False:\n",
    "            \n",
    "            #predicted value for the entire trajectory would be the mode of the predicted labels\n",
    "            y_hat = y_hat.groupby(['file_name','id']).apply(lambda group: pd.Series.mode(group['type'])[0])\n",
    "            y_test = y_val.groupby(['file_name','id']).first(['type'])\n",
    "        else:\n",
    "            \n",
    "            #predicted value for the entire trajectory would be the mode of the predicted labels\n",
    "            y_hat = y_hat.groupby(['file_name','id','edge_id']).apply(lambda group: pd.Series.mode(group['type'])[0])\n",
    "            y_test = y_val.groupby(['file_name','id','edge_id']).first(['type'])\n",
    "\n",
    "    if display:\n",
    "        y_hat_orig['id_traj'] = list(range(len(y_hat_orig)))\n",
    "        #y_hat_orig.set_index(['id_traj'], inplace = True,append = True)\n",
    "        \n",
    "        x_plot_num = 5\n",
    "        y_plot_num = int(sum(y_hat!=y_test)/x_plot_num) +1\n",
    "        fig, axes = plt.subplots(y_plot_num,x_plot_num, sharey = True, figsize=(5*x_plot_num,5*(y_plot_num)))\n",
    "        axes = axes.ravel()\n",
    "        i = 0\n",
    "        \n",
    "        for file_name,idx in X_val.index.droplevel((2)).unique():\n",
    "            if str(y_hat.loc[(file_name,idx)]) == str(y_test.loc[(file_name,idx)]):\n",
    "                continue\n",
    "            \n",
    "            axes[i].set(ylim=(0,1))\n",
    "            type_str = \"predicted: \"+str(y_hat.loc[(file_name,idx)]) + \", actual: \"+str(y_test.loc[(file_name,idx)])\n",
    "            sns.barplot(y = 'Car',x = 'id_traj',data = y_hat_orig.loc[(file_name,idx)],ax = axes[i]).set_title(\"file_name: \"+str(file_name)+\", id \"+str(idx)+\" \\n \"+type_str)\n",
    "            i+=1\n",
    "            \n",
    "        fig.tight_layout(h_pad=2)\n",
    "        #plt.show()\n",
    "        \n",
    "        \n",
    "    a = y_hat==y_test\n",
    "   \n",
    "    f = f1_score((y_test == 'Car').astype(int),(y_hat == 'Car').astype(int))\n",
    "   \n",
    "    return len(a[a==True]) / len(y_test),f\n",
    "            \n",
    "#val_voting_accuracy(X_val,y_val, model,predict_proba = True, display = True)       \n",
    "#plt.savefig('fig.png',dpi = 100)       \n",
    "\n",
    "def get_xy(df,overlap,traj_len,agg_dict,min_movement_limit = 0.75,outlier_limit=None,balance = None):\n",
    "    \n",
    "    df_agg =rolling_agg(df, window_size=traj_len, step=int((1 - overlap)*traj_len),agg_dict = agg_dict)\n",
    "    df_agg = df_agg[df_agg.speed_bool_count*min_movement_limit <= df_agg.speed_bool_sum]\n",
    "    df_agg.drop(['speed_bool_count','speed_bool_sum'],inplace= True,axis = 1)\n",
    "    if outlier_limit is not None:\n",
    "        df_agg = filter_by_percentile(df_agg,outlier_limit)\n",
    "    if balance == 'by_edge':\n",
    "        \n",
    "        df_agg['type_count'] = df_agg['type']\n",
    "        g_count = df_agg.groupby(['edge_id','type'], group_keys=False).count()['type_count']\n",
    "        g = df_agg.groupby(['type','edge_id'], group_keys=False)\n",
    "        df_agg = g.apply(lambda grp: grp.sample(min(g_count.loc[(grp.index.get_level_values(2)[0],slice(None))])))\n",
    "        df_agg.drop('type_count',inplace = True,axis = 1)\n",
    "        \n",
    "    if balance == 'by_type':\n",
    "        g = df_agg.groupby('type', group_keys=False)\n",
    "        df_agg = g.apply(lambda grp: grp.sample(g.size().min()))\n",
    "        \n",
    "    X,y = df_agg.drop('type', axis=1), df_agg.type\n",
    "    return X,y\n",
    "  \n",
    "def filter_by_percentile(df,percentile):\n",
    "    \n",
    "    top_le = 1-(percentile/100)\n",
    "    bottom_le = percentile/100\n",
    "    df_top = df.quantile(top_le).reset_index()\n",
    "    df_top['cond'] ='('+df_top['index']+\" <= \"+df_top[top_le].astype(str)+')'\n",
    "    df_bottom = df.quantile(bottom_le).reset_index()\n",
    "    df_bottom['cond'] ='('+df_bottom['index']+\" >= \"+df_bottom[bottom_le].astype(str)+')'\n",
    "    df = df.query(df_top.cond.str.cat(sep=' & '))\n",
    "    df = df.query(df_bottom.cond.str.cat(sep=' & '))\n",
    "    \n",
    "    return df  \n",
    "\n",
    "def __xtrack_dist_diff(df):\n",
    "    \"\"\"splits a vehicle trajectory into smaller trajectories of fixed size and removes\n",
    "    the last (len(df) mod size) riws\n",
    "    \"\"\"\n",
    "    \n",
    "    df[\"xtrack_diff\"] = df.loc[:,['xtrack_dist']]- df.loc[:,['xtrack_dist']].shift(-1)\n",
    "    df[\"xtrack_diff\"]=df['xtrack_diff'].fillna(0)\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "automated-buying",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = df.groupby(['file_name','id','edge_id'], as_index=False, group_keys=False) \\\n",
    "            .apply(__xtrack_dist_diff)\n",
    "df['xtrack_diff_sq'] = df['xtrack_diff']**2\n",
    "df['acc_edge_sq'] = df['acc_edge']**2\n",
    "df['acc_per_edge_sq'] = df['acc_per_edge']**2\n",
    "df['vehicle_density_by_lane'] = df['vehicle_density']/df['lanes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "pregnant-april",
   "metadata": {},
   "outputs": [],
   "source": [
    "class voting_model():\n",
    "    def __init__(self,model,X,y):\n",
    "        self.model = model\n",
    "        self.voting_model = self.fit(X,y)\n",
    "        \n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        \"\"\"fit quadratic weighted function on model output using X,y\"\"\"\n",
    "        \n",
    "        model_output = self.generate_op_df(X)\n",
    "        X_log = model_output.groupby(['file_name','id']).mean()\n",
    "        \n",
    "        Y_log = y.groupby(['file_name','id']).first(['type']).apply(lambda x: 1 if (self.model.classes_[0] == x) else -1 )\n",
    "        \n",
    "        model = LogisticRegression(penalty = 'none')\n",
    "        return model.fit(X_log,Y_log)\n",
    "    \n",
    "    \n",
    "    def generate_op_df(self,X):\n",
    "        model_output = pd.DataFrame(data = self.model.predict_proba(X)[:,0],index = X.index,columns = ['x_1'])\n",
    "        \n",
    "        model_output['x_2'] = model_output['x_1']**2\n",
    "        model_output['x_3'] = model_output['x_1']**3\n",
    "        model_output['const'] = 1\n",
    "        model_output['x_4'] = model_output['x_1']**4\n",
    "        \n",
    "        return model_output\n",
    "    \n",
    "    def predict(self,X):\n",
    "        model_output = self.generate_op_df(X)\n",
    "        X_test = model_output.groupby(['file_name','id']).mean()\n",
    "        model_output = self.voting_model.predict(X_test)\n",
    "        model_output = np.vectorize(lambda x: self.model.classes_[0] if (x>=0) else self.model.classes_[1])(model_output)\n",
    "        \n",
    "        return model_output\n",
    "    \n",
    "    def accuracy(self,X,y):\n",
    "        y_test = y.groupby(['file_name','id']).first(['type'])\n",
    "        y_hat = self.predict(X)\n",
    "        y_hat = pd.DataFrame(index = y_test.index,data = y_hat,columns = ['type'])\n",
    "        \n",
    "        #predicted value for the entire trajectory would be the mode of the predicted labels\n",
    "        #y_hat = y_hat.groupby(['file_name','id']).apply(lambda group: pd.Series.mode(group['type'])[0])\n",
    "        #y_test = y_val.groupby(['file_name','id']).first(['type'])\n",
    "        \n",
    "        a = y_hat['type']==y_test\n",
    "   \n",
    "        f = f1_score((y_test == 'Car').astype(int),(y_hat == 'Car').astype(int))\n",
    "        return len(a[a==True]) / len(y_test),f\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "level-sentence",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ensemble():\n",
    "    def __init__(self,model_num,accuracy_measure,model_list = None):\n",
    "        self.model_num = model_num\n",
    "        self.accuracy_measure = accuracy_measure\n",
    "        self.model_list = model_list\n",
    "        \n",
    "        \n",
    "    def find_ensemble(self,df_acc,traj_len,vehicle_density,predict_proba = False):\n",
    "        self.is_predict_proba = predict_proba\n",
    "        self.model_list = df_acc.loc[(slice(None),'accuracy','mean'),(vehicle_density,traj_len,self.accuracy_measure)].sort_values(ascending = False).index.get_level_values(0)[:self.model_num].to_list()\n",
    "      \n",
    "    def fit(self,X,y,model_dict=None):\n",
    "        self.model_dict = model_dict\n",
    "        \n",
    "        if model_dict == None:\n",
    "            self.model_dict = {}\n",
    "            for model in self.model_list:\n",
    "                self.model_dict[model] = model.fit(X,y)\n",
    "                \n",
    "        values_view = model_dict.values()\n",
    "        value_iterator = iter(values_view)\n",
    "        self.classes_ = next(value_iterator).classes_  \n",
    "                \n",
    "    \n",
    "    def predict(self,X):\n",
    "        label_list = []\n",
    "        df_model = pd.DataFrame(columns = self.model_list)\n",
    "        \n",
    "        if self.is_predict_proba == False:\n",
    "            for model in self.model_list:\n",
    "                df_model[model] = self.model_dict[model].predict(X)\n",
    "            return df_model.apply(lambda x : x.mode(),axis = 1)[0].to_numpy()\n",
    "            \n",
    "        else:\n",
    "            return self.predict_proba(X,get_label = True)\n",
    "    \n",
    "    def predict_proba(self,X,get_label = False):\n",
    "        label_list = []\n",
    "        model = list(self.model_dict.values())[0]\n",
    "        df_model = pd.DataFrame(columns = pd.MultiIndex.from_product([self.model_list,model.classes_]))#,index = np.arange(0,len(X)))\n",
    "        #df_model.loc[:,('MLP',model.classes_)] =  model.predict_proba(X)\n",
    "        for name in self.model_list:\n",
    "            model = self.model_dict[name]\n",
    "            \n",
    "            df_model.loc[:,(name,model.classes_)] = model.predict_proba(X)\n",
    "            \n",
    "        df_model = df_model.mean(axis=1, level=[1])\n",
    "        \n",
    "        if get_label == True:\n",
    "            return df_model.idxmax(axis=1).to_numpy()\n",
    "        else:\n",
    "            return df_model.to_numpy()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "assigned-desire",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initial parameters\n",
    "features_to_select = 10\n",
    "models = {\n",
    "        'Random Forest': Pipeline([('scaler', StandardScaler()), ('rf', RandomForestClassifier())]),\n",
    "        'AdaBoost':Pipeline([('scaler', StandardScaler()), ('abc', AdaBoostClassifier())]) ,\n",
    "        'SVM': Pipeline([('scaler', StandardScaler()), ('svc', SVC(max_iter=10000,probability = True))]) ,\n",
    "        'Log Regression': Pipeline([('scaler', StandardScaler()), ('lr', LogisticRegression(max_iter=10000))]) ,\n",
    "        'GBM': Pipeline([('scaler', StandardScaler()), ('gbm', GradientBoostingClassifier())]),\n",
    "        'MLP': Pipeline([('scaler', StandardScaler()), ('mlp', MLPClassifier(hidden_layer_sizes = (250,100,25),max_iter=1000,\\\n",
    "                                                                             learning_rate = 'adaptive',early_stopping = True,n_iter_no_change = 10))])\n",
    "                        \n",
    "    }\n",
    "\n",
    "\n",
    "df_acc = pd.DataFrame(index=pd.MultiIndex.from_product([models.keys(),['f1_score','accuracy'], ['mean']]))\n",
    "overlap = 0.7\n",
    "min_movement_limit = 1\n",
    "speed_limit = 0\n",
    "k = 5\n",
    "validation_ratio = 0.2\n",
    "kf = StratifiedKFold(n_splits=k, shuffle=True)\n",
    "accs = np.zeros(k)\n",
    "f1 = np.zeros(k)\n",
    "\n",
    "agg_dict = {'xtrack_diff': ['mean','std'],\n",
    "            'xtrack_dist': ['mean','std'],\n",
    "            'avg_surr_speed': ['mean','std'],\n",
    "            'lanes':['mean'],\n",
    "            'len':['mean'],\n",
    "            'speed':['mean','std'],\n",
    "            'speed_bool': ['count','sum'],\n",
    "            'acc_edge': ['mean','std'],\n",
    "            'acc_per_edge': ['mean','std']\n",
    "            \n",
    "            }\n",
    "            \n",
    "# agg_dict = {'xtrack_diff': ['mean','std','skew',pd.DataFrame.kurt],\n",
    "#             'xtrack_dist': ['mean','std','skew',pd.DataFrame.kurt],\n",
    "#             'avg_surr_speed': ['mean','std','skew',pd.DataFrame.kurt],\n",
    "#             'lanes':['mean'],\n",
    "#             'len':['mean'],\n",
    "#             'speed':['mean','std','skew',pd.DataFrame.kurt],\n",
    "#             'acc_edge': ['mean','std','skew',pd.DataFrame.kurt],\n",
    "#             'acc_per_edge': ['mean','std','skew',pd.DataFrame.kurt],\n",
    "#             'xtrack_diff_sq': ['mean','std','skew',pd.DataFrame.kurt],\n",
    "#             'acc_edge_sq': ['mean','std','skew',pd.DataFrame.kurt],\n",
    "#             'acc_per_edge_sq': ['mean','std','skew',pd.DataFrame.kurt],\n",
    "#             'vehicle_density_by_lane':['mean','std','skew',pd.DataFrame.kurt] }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "alpha-belle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of trajectories:  41055\n",
      "No of Car trajectories:  20539\n",
      "No of Taxi trajectories:  20516\n",
      "No of trajectories:  17204\n",
      "No of Car trajectories:  8608\n",
      "No of Taxi trajectories:  8596\n",
      "No of trajectories:  9760\n",
      "No of Car trajectories:  4884\n",
      "No of Taxi trajectories:  4876\n",
      "No of trajectories:  6192\n",
      "No of Car trajectories:  3096\n",
      "No of Taxi trajectories:  3096\n",
      "No of trajectories:  4181\n",
      "No of Car trajectories:  2133\n",
      "No of Taxi trajectories:  2048\n",
      "No of trajectories:  35709\n",
      "No of Car trajectories:  17864\n",
      "No of Car_1 trajectories:  17845\n",
      "No of trajectories:  15646\n",
      "No of Car trajectories:  7814\n",
      "No of Car_1 trajectories:  7832\n",
      "No of trajectories:  8350\n",
      "No of Car trajectories:  4175\n",
      "No of Car_1 trajectories:  4175\n",
      "No of trajectories:  5158\n",
      "No of Car trajectories:  2576\n",
      "No of Car_1 trajectories:  2582\n",
      "No of trajectories:  3490\n",
      "No of Car trajectories:  1745\n",
      "No of Car_1 trajectories:  1745\n"
     ]
    }
   ],
   "source": [
    "# Car and Taxi classification\n",
    "#plt.ioff()\n",
    "traj_lens = np.arange(50,300, step=50)\n",
    "df_acc = pd.DataFrame(columns = pd.MultiIndex.from_product([[1],traj_lens,['test','val_mean','val_log_voting']]),index=pd.MultiIndex.from_product([models.keys(),['accuracy','accuracy_baseline'], ['mean']]))\n",
    "ensemble_2 = ensemble(2,'test')\n",
    "ensemble_3 = ensemble(3,'test')\n",
    "ensemble_5 = ensemble(5,'test')\n",
    "validation_ratio = 0.2\n",
    "test_ratio = 0.2\n",
    "pca = PCA(n_components=5)\n",
    "is_pca = False\n",
    "vehicle_density = 1\n",
    "is_log_model_voting = True\n",
    "\n",
    "for vehicle in ['Taxi','Car_1']:\n",
    "    \n",
    "    if vehicle == 'Car_1':\n",
    "        df_type = df[df.type == 'Car']\n",
    "        accuracy_metric = 'accuracy_baseline'\n",
    "    else : \n",
    "        df_type = df.copy()\n",
    "        accuracy_metric = 'accuracy'\n",
    "        \n",
    "    for traj_len in traj_lens:\n",
    "\n",
    "        df_filtered = df_type.groupby(df_type.index.names[:-1]) \\\n",
    "                .filter(lambda grp: (len(grp) >= traj_len) )\n",
    "        \n",
    "        df_filtered['speed_bool'] = (df_filtered['speed']>speed_limit).astype(int)\n",
    "        \n",
    "        if vehicle == 'Car_1':\n",
    "            #sample 50% of cars and label them as car_1\n",
    "            df_index = df_filtered.reset_index()[['file_name','id']].drop_duplicates()\n",
    "            df_filtered.loc[df_filtered.reset_index(['edge_id', 'time'],drop = True).index.isin(df_index.sample(frac = 0.5).set_index(['file_name','id']).index),'type']=vehicle\n",
    " \n",
    "        df_train_test,df_val = validation_set(df_filtered,validation_ratio)\n",
    "        df_train,df_test = validation_set(df_train_test,test_ratio)\n",
    "\n",
    "        #aggregate trajectories\n",
    "        #X,y = get_xy(df_train_test,overlap,traj_len,agg_dict,1)\n",
    "        X_train,y_train = get_xy(df_train,overlap,traj_len,agg_dict,outlier_limit = 1,balance = 'by_edge')\n",
    "        X_test,y_test = get_xy(df_test,overlap,traj_len,agg_dict,balance = 'by_type')\n",
    "        X_test_voting,y_test_voting = get_xy(df_test,overlap,traj_len,agg_dict)\n",
    "        X_val,y_val = get_xy(df_val,overlap,traj_len,agg_dict)\n",
    "\n",
    "        if is_pca:\n",
    "            pca.fit(X_train)\n",
    "            X_test_voting = pd.DataFrame(data = pca.transform(X_test_voting),index = X_test_voting.index)\n",
    "            X_train = pd.DataFrame(data = pca.transform(X_train),index = X_train.index)\n",
    "            X_test = pd.DataFrame(data = pca.transform(X_test),index = X_test.index)\n",
    "            X_val = pd.DataFrame(data = pca.transform(X_val),index = X_val.index)\n",
    "\n",
    "        #store percent cars and taxis\n",
    "        print(\"No of trajectories: \",len(X_train))\n",
    "        print(\"No of Car trajectories: \",sum(y_train == 'Car'))\n",
    "        print(\"No of \"+vehicle+\" trajectories: \",sum(y_train == vehicle))\n",
    "        \n",
    "        df_acc.loc[('traj_len','Car_'+vehicle,'total'), (vehicle_density,traj_len,'test')] = len(X_test)\n",
    "        df_acc.loc[('traj_len','Car_'+vehicle+'_percent','Car'), (vehicle_density,traj_len,'test')] = sum(y_test == 'Car')/len(X_test)\n",
    "        df_acc.loc[('traj_len','Car_'+vehicle+'_percent',vehicle), (vehicle_density,traj_len,'test')] = sum(y_test == vehicle)/ len(X_test)\n",
    "\n",
    "        woedge_count = y_val.reset_index(['edge_id'],drop = True).reset_index().drop_duplicates()\n",
    "        df_acc.loc[('traj_len','Car_'+vehicle,'total'), (vehicle_density,traj_len,'val_mean')] = len(woedge_count)\n",
    "        df_acc.loc[('traj_len','Car_'+vehicle+'_percent','Car'), (vehicle_density,traj_len,'val_mean')] = sum(woedge_count.type == 'Car')/len(woedge_count)\n",
    "        df_acc.loc[('traj_len','Car_'+vehicle+'_percent',vehicle), (vehicle_density,traj_len,'val_mean')] =sum(woedge_count.type == vehicle)/len(woedge_count)\n",
    "\n",
    "        #by_edge_count = y_val.reset_index().drop_duplicates()\n",
    "        df_acc.loc[('traj_len','Car_'+vehicle,'total'), (vehicle_density,traj_len,'val_log_voting')] = len(woedge_count)\n",
    "        df_acc.loc[('traj_len','Car_'+vehicle+'_percent','Car'), (vehicle_density,traj_len,'val_log_voting')] = sum(woedge_count.type == 'Car')/len(woedge_count)\n",
    "        df_acc.loc[('traj_len','Car_'+vehicle+'_percent',vehicle), (vehicle_density,traj_len,'val_log_voting')] = sum(woedge_count.type == vehicle)/len(woedge_count)\n",
    "\n",
    "        model_dict = {}\n",
    "        \n",
    "        # fit different models\n",
    "        for name, model in models.items():\n",
    "\n",
    "            #fit the model on training set\n",
    "            model.fit(X_train,y_train)\n",
    "\n",
    "            #test the model on testing set and save accuracy estimate as test (this accuracy estimate will be used to find ensemble) \n",
    "            val_accs,val_f1 = train_and_accuracy(X_test,y_test,model)                                   \n",
    "            df_acc.loc[(name, accuracy_metric,'mean'),  (vehicle_density,traj_len,'test')] = round(100*val_accs, 3)\n",
    "            \n",
    "            #find accuracy of the model on validation set with voting using mean\n",
    "            val_accs,val_f1 = val_voting_accuracy(X_val,y_val, model,predict_proba = True)\n",
    "            df_acc.loc[(name, accuracy_metric,'mean'), (vehicle_density,traj_len,'val_mean')] = round(100*val_accs, 3)\n",
    "            #plt.savefig(\"traj_len\"+str(traj_len)+name+\".png\")\n",
    "            \n",
    "            if is_log_model_voting:\n",
    "            #train logistic regression for voting using the test set and training model\n",
    "                voting_m = voting_model(model,X_test_voting,y_test_voting)\n",
    "            #find the accuracy of the model on validation set with voting using logistic regression\n",
    "                val_accs,val_f1 = voting_m.accuracy(X_val,y_val)#, voting_m, predict_proba = False)\n",
    "                df_acc.loc[(name, accuracy_metric,'mean'), (vehicle_density,traj_len,'val_log_voting')] = round(100*val_accs, 3)\n",
    "    \n",
    "            #save model in dictionary for ensemble\n",
    "            model_dict[name] = model\n",
    "\n",
    "        #generate ensembles with 2,3 and 5 models\n",
    "        ensemble_2.find_ensemble(df_acc,traj_len,vehicle_density,True)\n",
    "        ensemble_2.fit(X_train,y_train,model_dict)\n",
    "        ensemble_3.find_ensemble(df_acc,traj_len,vehicle_density,True)\n",
    "        ensemble_3.fit(X_train,y_train,model_dict)\n",
    "        ensemble_5.find_ensemble(df_acc,traj_len,vehicle_density,True)\n",
    "        ensemble_5.fit(X_train,y_train,model_dict)\n",
    "\n",
    "        #test accuracy of ensembles on validation set with mean \n",
    "        val_accs,val_f1 = val_voting_accuracy(X_val,y_val, ensemble_2)\n",
    "        df_acc.loc[('ensemble_2', accuracy_metric,'mean'), (vehicle_density,traj_len,'val_mean')] = round(100*val_accs, 3)\n",
    "        val_accs,val_f1 = val_voting_accuracy(X_val,y_val, ensemble_3)\n",
    "        df_acc.loc[('ensemble_3', accuracy_metric,'mean'), (vehicle_density,traj_len,'val_mean')] = round(100*val_accs, 3)\n",
    "        val_accs,val_f1 = val_voting_accuracy(X_val,y_val, ensemble_5)\n",
    "        df_acc.loc[('ensemble_5', accuracy_metric,'mean'), (vehicle_density,traj_len,'val_mean')] = round(100*val_accs, 3)\n",
    "\n",
    "        if is_log_model_voting:\n",
    "            #test accuracy of ensembles on validation using logistic voting (trained on testing set)\n",
    "            voting_m = voting_model(ensemble_2,X_test_voting,y_test_voting)\n",
    "            val_accs,val_f1 = voting_m.accuracy(X_val,y_val)\n",
    "            df_acc.loc[('ensemble_2', accuracy_metric,'mean'), (vehicle_density,traj_len,'val_log_voting')] = round(100*val_accs, 3)\n",
    "            voting_m = voting_model(ensemble_3,X_test_voting,y_test_voting)\n",
    "            val_accs,val_f1 = voting_m.accuracy(X_val,y_val)\n",
    "            df_acc.loc[('ensemble_3', accuracy_metric,'mean'), (vehicle_density,traj_len,'val_log_voting')] = round(100*val_accs, 3)\n",
    "            voting_m = voting_model(ensemble_5,X_test_voting,y_test_voting)\n",
    "            val_accs,val_f1 = voting_m.accuracy(X_val,y_val)\n",
    "            df_acc.loc[('ensemble_5', accuracy_metric,'mean'), (vehicle_density,traj_len,'val_log_voting')] = round(100*val_accs, 3)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "appreciated-disposition",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"15\" halign=\"left\">1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">50</th>\n",
       "      <th colspan=\"3\" halign=\"left\">100</th>\n",
       "      <th colspan=\"3\" halign=\"left\">150</th>\n",
       "      <th colspan=\"3\" halign=\"left\">200</th>\n",
       "      <th colspan=\"3\" halign=\"left\">250</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>test</th>\n",
       "      <th>val_mean</th>\n",
       "      <th>val_log_voting</th>\n",
       "      <th>test</th>\n",
       "      <th>val_mean</th>\n",
       "      <th>val_log_voting</th>\n",
       "      <th>test</th>\n",
       "      <th>val_mean</th>\n",
       "      <th>val_log_voting</th>\n",
       "      <th>test</th>\n",
       "      <th>val_mean</th>\n",
       "      <th>val_log_voting</th>\n",
       "      <th>test</th>\n",
       "      <th>val_mean</th>\n",
       "      <th>val_log_voting</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">AdaBoost</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>mean</th>\n",
       "      <td>57.216</td>\n",
       "      <td>57.966</td>\n",
       "      <td>57.966</td>\n",
       "      <td>55.745</td>\n",
       "      <td>53.71</td>\n",
       "      <td>57.597</td>\n",
       "      <td>50.978</td>\n",
       "      <td>56.25</td>\n",
       "      <td>55.515</td>\n",
       "      <td>58.166</td>\n",
       "      <td>55.645</td>\n",
       "      <td>48.79</td>\n",
       "      <td>54.461</td>\n",
       "      <td>49.776</td>\n",
       "      <td>52.915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy_baseline</th>\n",
       "      <th>mean</th>\n",
       "      <td>51.148</td>\n",
       "      <td>49.128</td>\n",
       "      <td>49.128</td>\n",
       "      <td>49.978</td>\n",
       "      <td>43.574</td>\n",
       "      <td>43.26</td>\n",
       "      <td>50.937</td>\n",
       "      <td>46.128</td>\n",
       "      <td>52.862</td>\n",
       "      <td>53.177</td>\n",
       "      <td>45.652</td>\n",
       "      <td>49.275</td>\n",
       "      <td>49.653</td>\n",
       "      <td>53.755</td>\n",
       "      <td>46.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">GBM</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>mean</th>\n",
       "      <td>59.267</td>\n",
       "      <td>62.712</td>\n",
       "      <td>63.729</td>\n",
       "      <td>59.414</td>\n",
       "      <td>58.657</td>\n",
       "      <td>60.071</td>\n",
       "      <td>53.425</td>\n",
       "      <td>59.926</td>\n",
       "      <td>60.294</td>\n",
       "      <td>58.92</td>\n",
       "      <td>59.274</td>\n",
       "      <td>57.258</td>\n",
       "      <td>53.734</td>\n",
       "      <td>59.193</td>\n",
       "      <td>56.951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy_baseline</th>\n",
       "      <th>mean</th>\n",
       "      <td>49.972</td>\n",
       "      <td>48.547</td>\n",
       "      <td>49.128</td>\n",
       "      <td>49.144</td>\n",
       "      <td>43.26</td>\n",
       "      <td>48.276</td>\n",
       "      <td>52.853</td>\n",
       "      <td>46.465</td>\n",
       "      <td>47.475</td>\n",
       "      <td>52.118</td>\n",
       "      <td>46.377</td>\n",
       "      <td>49.275</td>\n",
       "      <td>48.958</td>\n",
       "      <td>50.198</td>\n",
       "      <td>50.198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Log Regression</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>mean</th>\n",
       "      <td>53.209</td>\n",
       "      <td>55.932</td>\n",
       "      <td>61.356</td>\n",
       "      <td>55.491</td>\n",
       "      <td>54.417</td>\n",
       "      <td>54.064</td>\n",
       "      <td>52.838</td>\n",
       "      <td>55.147</td>\n",
       "      <td>52.574</td>\n",
       "      <td>47.676</td>\n",
       "      <td>56.855</td>\n",
       "      <td>51.613</td>\n",
       "      <td>50.207</td>\n",
       "      <td>46.637</td>\n",
       "      <td>53.812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy_baseline</th>\n",
       "      <th>mean</th>\n",
       "      <td>50.678</td>\n",
       "      <td>54.07</td>\n",
       "      <td>58.14</td>\n",
       "      <td>50.176</td>\n",
       "      <td>44.828</td>\n",
       "      <td>44.828</td>\n",
       "      <td>55.707</td>\n",
       "      <td>48.485</td>\n",
       "      <td>48.148</td>\n",
       "      <td>52.723</td>\n",
       "      <td>47.826</td>\n",
       "      <td>45.652</td>\n",
       "      <td>47.222</td>\n",
       "      <td>52.174</td>\n",
       "      <td>51.383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">MLP</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>mean</th>\n",
       "      <td>56.429</td>\n",
       "      <td>59.661</td>\n",
       "      <td>62.034</td>\n",
       "      <td>58.56</td>\n",
       "      <td>62.544</td>\n",
       "      <td>62.544</td>\n",
       "      <td>52.055</td>\n",
       "      <td>63.235</td>\n",
       "      <td>60.662</td>\n",
       "      <td>57.538</td>\n",
       "      <td>60.081</td>\n",
       "      <td>59.677</td>\n",
       "      <td>55.083</td>\n",
       "      <td>57.399</td>\n",
       "      <td>57.848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy_baseline</th>\n",
       "      <th>mean</th>\n",
       "      <td>50.141</td>\n",
       "      <td>47.093</td>\n",
       "      <td>48.547</td>\n",
       "      <td>51.91</td>\n",
       "      <td>43.574</td>\n",
       "      <td>43.26</td>\n",
       "      <td>51.363</td>\n",
       "      <td>53.872</td>\n",
       "      <td>50.842</td>\n",
       "      <td>50.908</td>\n",
       "      <td>46.377</td>\n",
       "      <td>46.739</td>\n",
       "      <td>49.19</td>\n",
       "      <td>48.617</td>\n",
       "      <td>50.988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Random Forest</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>mean</th>\n",
       "      <td>57.428</td>\n",
       "      <td>62.712</td>\n",
       "      <td>64.407</td>\n",
       "      <td>57.568</td>\n",
       "      <td>61.131</td>\n",
       "      <td>60.424</td>\n",
       "      <td>51.663</td>\n",
       "      <td>59.559</td>\n",
       "      <td>61.765</td>\n",
       "      <td>58.731</td>\n",
       "      <td>58.468</td>\n",
       "      <td>58.871</td>\n",
       "      <td>53.527</td>\n",
       "      <td>56.951</td>\n",
       "      <td>56.054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy_baseline</th>\n",
       "      <th>mean</th>\n",
       "      <td>50.414</td>\n",
       "      <td>46.221</td>\n",
       "      <td>47.965</td>\n",
       "      <td>50.878</td>\n",
       "      <td>44.828</td>\n",
       "      <td>50.47</td>\n",
       "      <td>53.407</td>\n",
       "      <td>47.811</td>\n",
       "      <td>52.862</td>\n",
       "      <td>52.042</td>\n",
       "      <td>47.826</td>\n",
       "      <td>50.725</td>\n",
       "      <td>48.843</td>\n",
       "      <td>47.036</td>\n",
       "      <td>53.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">SVM</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>mean</th>\n",
       "      <td>54.952</td>\n",
       "      <td>53.559</td>\n",
       "      <td>62.712</td>\n",
       "      <td>56.784</td>\n",
       "      <td>59.717</td>\n",
       "      <td>57.597</td>\n",
       "      <td>54.648</td>\n",
       "      <td>61.029</td>\n",
       "      <td>62.868</td>\n",
       "      <td>59.548</td>\n",
       "      <td>60.887</td>\n",
       "      <td>55.242</td>\n",
       "      <td>54.461</td>\n",
       "      <td>56.502</td>\n",
       "      <td>55.157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy_baseline</th>\n",
       "      <th>mean</th>\n",
       "      <td>50.781</td>\n",
       "      <td>49.128</td>\n",
       "      <td>50.872</td>\n",
       "      <td>49.1</td>\n",
       "      <td>44.514</td>\n",
       "      <td>48.589</td>\n",
       "      <td>55.451</td>\n",
       "      <td>50.168</td>\n",
       "      <td>52.189</td>\n",
       "      <td>51.967</td>\n",
       "      <td>47.101</td>\n",
       "      <td>45.652</td>\n",
       "      <td>44.792</td>\n",
       "      <td>43.478</td>\n",
       "      <td>55.336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">ensemble_2</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>66.102</td>\n",
       "      <td>64.746</td>\n",
       "      <td>NaN</td>\n",
       "      <td>60.424</td>\n",
       "      <td>63.604</td>\n",
       "      <td>NaN</td>\n",
       "      <td>59.926</td>\n",
       "      <td>61.765</td>\n",
       "      <td>NaN</td>\n",
       "      <td>61.29</td>\n",
       "      <td>60.887</td>\n",
       "      <td>NaN</td>\n",
       "      <td>60.538</td>\n",
       "      <td>58.296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy_baseline</th>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>47.674</td>\n",
       "      <td>46.221</td>\n",
       "      <td>NaN</td>\n",
       "      <td>44.514</td>\n",
       "      <td>41.379</td>\n",
       "      <td>NaN</td>\n",
       "      <td>49.832</td>\n",
       "      <td>50.168</td>\n",
       "      <td>NaN</td>\n",
       "      <td>45.29</td>\n",
       "      <td>46.014</td>\n",
       "      <td>NaN</td>\n",
       "      <td>47.826</td>\n",
       "      <td>47.431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">ensemble_3</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>66.102</td>\n",
       "      <td>64.746</td>\n",
       "      <td>NaN</td>\n",
       "      <td>61.484</td>\n",
       "      <td>61.837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>59.559</td>\n",
       "      <td>59.559</td>\n",
       "      <td>NaN</td>\n",
       "      <td>58.468</td>\n",
       "      <td>57.258</td>\n",
       "      <td>NaN</td>\n",
       "      <td>61.883</td>\n",
       "      <td>48.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy_baseline</th>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>46.802</td>\n",
       "      <td>48.547</td>\n",
       "      <td>NaN</td>\n",
       "      <td>46.395</td>\n",
       "      <td>42.947</td>\n",
       "      <td>NaN</td>\n",
       "      <td>48.148</td>\n",
       "      <td>51.515</td>\n",
       "      <td>NaN</td>\n",
       "      <td>48.913</td>\n",
       "      <td>45.652</td>\n",
       "      <td>NaN</td>\n",
       "      <td>45.455</td>\n",
       "      <td>52.569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">ensemble_5</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>63.729</td>\n",
       "      <td>64.746</td>\n",
       "      <td>NaN</td>\n",
       "      <td>62.544</td>\n",
       "      <td>59.717</td>\n",
       "      <td>NaN</td>\n",
       "      <td>59.559</td>\n",
       "      <td>62.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>60.081</td>\n",
       "      <td>54.839</td>\n",
       "      <td>NaN</td>\n",
       "      <td>59.641</td>\n",
       "      <td>52.018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy_baseline</th>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>48.256</td>\n",
       "      <td>44.767</td>\n",
       "      <td>NaN</td>\n",
       "      <td>43.26</td>\n",
       "      <td>41.693</td>\n",
       "      <td>NaN</td>\n",
       "      <td>49.495</td>\n",
       "      <td>50.168</td>\n",
       "      <td>NaN</td>\n",
       "      <td>47.101</td>\n",
       "      <td>47.826</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42.688</td>\n",
       "      <td>49.407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">traj_len</th>\n",
       "      <th>Car_Car_1</th>\n",
       "      <th>total</th>\n",
       "      <td>10624</td>\n",
       "      <td>344</td>\n",
       "      <td>344</td>\n",
       "      <td>4554</td>\n",
       "      <td>319</td>\n",
       "      <td>319</td>\n",
       "      <td>2348</td>\n",
       "      <td>297</td>\n",
       "      <td>297</td>\n",
       "      <td>1322</td>\n",
       "      <td>276</td>\n",
       "      <td>276</td>\n",
       "      <td>864</td>\n",
       "      <td>253</td>\n",
       "      <td>253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Car_Car_1_percent</th>\n",
       "      <th>Car</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.502907</td>\n",
       "      <td>0.502907</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.498433</td>\n",
       "      <td>0.498433</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.505051</td>\n",
       "      <td>0.505051</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.501976</td>\n",
       "      <td>0.501976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Car_1</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.497093</td>\n",
       "      <td>0.497093</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.501567</td>\n",
       "      <td>0.501567</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.494949</td>\n",
       "      <td>0.494949</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.498024</td>\n",
       "      <td>0.498024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Car_Taxi</th>\n",
       "      <th>total</th>\n",
       "      <td>9410</td>\n",
       "      <td>295</td>\n",
       "      <td>295</td>\n",
       "      <td>4334</td>\n",
       "      <td>283</td>\n",
       "      <td>283</td>\n",
       "      <td>2044</td>\n",
       "      <td>272</td>\n",
       "      <td>272</td>\n",
       "      <td>1592</td>\n",
       "      <td>248</td>\n",
       "      <td>248</td>\n",
       "      <td>964</td>\n",
       "      <td>223</td>\n",
       "      <td>223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Car_Taxi_percent</th>\n",
       "      <th>Car</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.498305</td>\n",
       "      <td>0.498305</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.501767</td>\n",
       "      <td>0.501767</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.495968</td>\n",
       "      <td>0.495968</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.488789</td>\n",
       "      <td>0.488789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Taxi</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.501695</td>\n",
       "      <td>0.501695</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.498233</td>\n",
       "      <td>0.498233</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.504032</td>\n",
       "      <td>0.504032</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.511211</td>\n",
       "      <td>0.511211</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             1                           \\\n",
       "                                           50                             \n",
       "                                          test  val_mean val_log_voting   \n",
       "AdaBoost       accuracy          mean   57.216    57.966         57.966   \n",
       "               accuracy_baseline mean   51.148    49.128         49.128   \n",
       "GBM            accuracy          mean   59.267    62.712         63.729   \n",
       "               accuracy_baseline mean   49.972    48.547         49.128   \n",
       "Log Regression accuracy          mean   53.209    55.932         61.356   \n",
       "               accuracy_baseline mean   50.678     54.07          58.14   \n",
       "MLP            accuracy          mean   56.429    59.661         62.034   \n",
       "               accuracy_baseline mean   50.141    47.093         48.547   \n",
       "Random Forest  accuracy          mean   57.428    62.712         64.407   \n",
       "               accuracy_baseline mean   50.414    46.221         47.965   \n",
       "SVM            accuracy          mean   54.952    53.559         62.712   \n",
       "               accuracy_baseline mean   50.781    49.128         50.872   \n",
       "ensemble_2     accuracy          mean      NaN    66.102         64.746   \n",
       "               accuracy_baseline mean      NaN    47.674         46.221   \n",
       "ensemble_3     accuracy          mean      NaN    66.102         64.746   \n",
       "               accuracy_baseline mean      NaN    46.802         48.547   \n",
       "ensemble_5     accuracy          mean      NaN    63.729         64.746   \n",
       "               accuracy_baseline mean      NaN    48.256         44.767   \n",
       "traj_len       Car_Car_1         total   10624       344            344   \n",
       "               Car_Car_1_percent Car       0.5  0.502907       0.502907   \n",
       "                                 Car_1     0.5  0.497093       0.497093   \n",
       "               Car_Taxi          total    9410       295            295   \n",
       "               Car_Taxi_percent  Car       0.5  0.498305       0.498305   \n",
       "                                 Taxi      0.5  0.501695       0.501695   \n",
       "\n",
       "                                                                         \\\n",
       "                                           100                            \n",
       "                                          test  val_mean val_log_voting   \n",
       "AdaBoost       accuracy          mean   55.745     53.71         57.597   \n",
       "               accuracy_baseline mean   49.978    43.574          43.26   \n",
       "GBM            accuracy          mean   59.414    58.657         60.071   \n",
       "               accuracy_baseline mean   49.144     43.26         48.276   \n",
       "Log Regression accuracy          mean   55.491    54.417         54.064   \n",
       "               accuracy_baseline mean   50.176    44.828         44.828   \n",
       "MLP            accuracy          mean    58.56    62.544         62.544   \n",
       "               accuracy_baseline mean    51.91    43.574          43.26   \n",
       "Random Forest  accuracy          mean   57.568    61.131         60.424   \n",
       "               accuracy_baseline mean   50.878    44.828          50.47   \n",
       "SVM            accuracy          mean   56.784    59.717         57.597   \n",
       "               accuracy_baseline mean     49.1    44.514         48.589   \n",
       "ensemble_2     accuracy          mean      NaN    60.424         63.604   \n",
       "               accuracy_baseline mean      NaN    44.514         41.379   \n",
       "ensemble_3     accuracy          mean      NaN    61.484         61.837   \n",
       "               accuracy_baseline mean      NaN    46.395         42.947   \n",
       "ensemble_5     accuracy          mean      NaN    62.544         59.717   \n",
       "               accuracy_baseline mean      NaN     43.26         41.693   \n",
       "traj_len       Car_Car_1         total    4554       319            319   \n",
       "               Car_Car_1_percent Car       0.5  0.498433       0.498433   \n",
       "                                 Car_1     0.5  0.501567       0.501567   \n",
       "               Car_Taxi          total    4334       283            283   \n",
       "               Car_Taxi_percent  Car       0.5  0.501767       0.501767   \n",
       "                                 Taxi      0.5  0.498233       0.498233   \n",
       "\n",
       "                                                                         \\\n",
       "                                           150                            \n",
       "                                          test  val_mean val_log_voting   \n",
       "AdaBoost       accuracy          mean   50.978     56.25         55.515   \n",
       "               accuracy_baseline mean   50.937    46.128         52.862   \n",
       "GBM            accuracy          mean   53.425    59.926         60.294   \n",
       "               accuracy_baseline mean   52.853    46.465         47.475   \n",
       "Log Regression accuracy          mean   52.838    55.147         52.574   \n",
       "               accuracy_baseline mean   55.707    48.485         48.148   \n",
       "MLP            accuracy          mean   52.055    63.235         60.662   \n",
       "               accuracy_baseline mean   51.363    53.872         50.842   \n",
       "Random Forest  accuracy          mean   51.663    59.559         61.765   \n",
       "               accuracy_baseline mean   53.407    47.811         52.862   \n",
       "SVM            accuracy          mean   54.648    61.029         62.868   \n",
       "               accuracy_baseline mean   55.451    50.168         52.189   \n",
       "ensemble_2     accuracy          mean      NaN    59.926         61.765   \n",
       "               accuracy_baseline mean      NaN    49.832         50.168   \n",
       "ensemble_3     accuracy          mean      NaN    59.559         59.559   \n",
       "               accuracy_baseline mean      NaN    48.148         51.515   \n",
       "ensemble_5     accuracy          mean      NaN    59.559           62.5   \n",
       "               accuracy_baseline mean      NaN    49.495         50.168   \n",
       "traj_len       Car_Car_1         total    2348       297            297   \n",
       "               Car_Car_1_percent Car       0.5  0.505051       0.505051   \n",
       "                                 Car_1     0.5  0.494949       0.494949   \n",
       "               Car_Taxi          total    2044       272            272   \n",
       "               Car_Taxi_percent  Car       0.5       0.5            0.5   \n",
       "                                 Taxi      0.5       0.5            0.5   \n",
       "\n",
       "                                                                         \\\n",
       "                                           200                            \n",
       "                                          test  val_mean val_log_voting   \n",
       "AdaBoost       accuracy          mean   58.166    55.645          48.79   \n",
       "               accuracy_baseline mean   53.177    45.652         49.275   \n",
       "GBM            accuracy          mean    58.92    59.274         57.258   \n",
       "               accuracy_baseline mean   52.118    46.377         49.275   \n",
       "Log Regression accuracy          mean   47.676    56.855         51.613   \n",
       "               accuracy_baseline mean   52.723    47.826         45.652   \n",
       "MLP            accuracy          mean   57.538    60.081         59.677   \n",
       "               accuracy_baseline mean   50.908    46.377         46.739   \n",
       "Random Forest  accuracy          mean   58.731    58.468         58.871   \n",
       "               accuracy_baseline mean   52.042    47.826         50.725   \n",
       "SVM            accuracy          mean   59.548    60.887         55.242   \n",
       "               accuracy_baseline mean   51.967    47.101         45.652   \n",
       "ensemble_2     accuracy          mean      NaN     61.29         60.887   \n",
       "               accuracy_baseline mean      NaN     45.29         46.014   \n",
       "ensemble_3     accuracy          mean      NaN    58.468         57.258   \n",
       "               accuracy_baseline mean      NaN    48.913         45.652   \n",
       "ensemble_5     accuracy          mean      NaN    60.081         54.839   \n",
       "               accuracy_baseline mean      NaN    47.101         47.826   \n",
       "traj_len       Car_Car_1         total    1322       276            276   \n",
       "               Car_Car_1_percent Car       0.5       0.5            0.5   \n",
       "                                 Car_1     0.5       0.5            0.5   \n",
       "               Car_Taxi          total    1592       248            248   \n",
       "               Car_Taxi_percent  Car       0.5  0.495968       0.495968   \n",
       "                                 Taxi      0.5  0.504032       0.504032   \n",
       "\n",
       "                                                                         \n",
       "                                           250                           \n",
       "                                          test  val_mean val_log_voting  \n",
       "AdaBoost       accuracy          mean   54.461    49.776         52.915  \n",
       "               accuracy_baseline mean   49.653    53.755          46.64  \n",
       "GBM            accuracy          mean   53.734    59.193         56.951  \n",
       "               accuracy_baseline mean   48.958    50.198         50.198  \n",
       "Log Regression accuracy          mean   50.207    46.637         53.812  \n",
       "               accuracy_baseline mean   47.222    52.174         51.383  \n",
       "MLP            accuracy          mean   55.083    57.399         57.848  \n",
       "               accuracy_baseline mean    49.19    48.617         50.988  \n",
       "Random Forest  accuracy          mean   53.527    56.951         56.054  \n",
       "               accuracy_baseline mean   48.843    47.036          53.36  \n",
       "SVM            accuracy          mean   54.461    56.502         55.157  \n",
       "               accuracy_baseline mean   44.792    43.478         55.336  \n",
       "ensemble_2     accuracy          mean      NaN    60.538         58.296  \n",
       "               accuracy_baseline mean      NaN    47.826         47.431  \n",
       "ensemble_3     accuracy          mean      NaN    61.883          48.43  \n",
       "               accuracy_baseline mean      NaN    45.455         52.569  \n",
       "ensemble_5     accuracy          mean      NaN    59.641         52.018  \n",
       "               accuracy_baseline mean      NaN    42.688         49.407  \n",
       "traj_len       Car_Car_1         total     864       253            253  \n",
       "               Car_Car_1_percent Car       0.5  0.501976       0.501976  \n",
       "                                 Car_1     0.5  0.498024       0.498024  \n",
       "               Car_Taxi          total     964       223            223  \n",
       "               Car_Taxi_percent  Car       0.5  0.488789       0.488789  \n",
       "                                 Taxi      0.5  0.511211       0.511211  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_acc.sort_index()#.to_csv(\"accuracy_block4_100_traj_len.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handmade-carnival",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
